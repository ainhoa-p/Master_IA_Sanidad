{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN From Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/NN_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP95YI3lgIrd"
      },
      "source": [
        "## Programando una red neuronal desde cero\n",
        "\n",
        "En este ejemplo veremos como programar una red neuronal desde 0. Empezaremos con el problema que nos encontramos anteriormente que no podía resolver nuestro perceptrón, la clasificación del dataset de MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nwOH7wuUpD_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Utilizamos la función load_digits para descargarnos los datos de mnist\n",
        "digits = load_digits()\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mPaQ6amB78c",
        "outputId": "b50a6ae4-fd18-4d46-bfb3-ab0b7249071b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "#Vamos a echar un ojo a lo que nos hemos descargado\n",
        "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
        "for ax, image, label in zip(axes, digits.images, digits.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    ax.set_title('Training: %i' % label)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x216 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMxvgMDSB9aE",
        "outputId": "5dcc692b-1bb7-4269-edf5-641351f82175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Preparemos nuestro set de entrenamiento\n",
        "n_samples = len(digits.images)\n",
        "data = digits.images.reshape((n_samples, -1))\n",
        "# Imprimimos el tamaño de data.shape para sabe el número de pixeles que tenemos en cada muestra\n",
        "print(data.shape)\n",
        "# Dividimos en training y test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, shuffle=False)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wASFGvwpgsMB"
      },
      "source": [
        "## Red neuronal de 4 capas en numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IucezTTXiSM"
      },
      "source": [
        "class DeepNeuralNetwork():\n",
        "    def __init__(self, sizes, epochs=10, l_rate=0.03):\n",
        "        self.sizes = sizes\n",
        "        self.epochs = epochs\n",
        "        self.l_rate = l_rate\n",
        "\n",
        "        # Ahora tenemos más parámetros, así que los organizamos en un diccionario\n",
        "        self.params = self.initialization()\n",
        "\n",
        "    def initialization(self):\n",
        "        # Número de neuronas por capa (argumento de entrada)\n",
        "        input_layer=self.sizes[0]\n",
        "        hidden_1=self.sizes[1]\n",
        "        hidden_2=self.sizes[2]\n",
        "        output_layer=self.sizes[3]\n",
        "\n",
        "        params = {\n",
        "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
        "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
        "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
        "        }\n",
        "\n",
        "        return params\n",
        "\n",
        "    def sigmoid(self, x, derivative=False):\n",
        "      # Función de activación sigmoide\n",
        "        if derivative:\n",
        "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x, derivative=False):\n",
        "        # Función de activación softmax\n",
        "        exps = np.exp(x - x.max())\n",
        "        if derivative:\n",
        "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "    def forward_pass(self, x_train):\n",
        "        params = self.params\n",
        "\n",
        "        # input layer activations becomes sample\n",
        "        params['A0'] = x_train\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "        params['A1'] = self.sigmoid(params['Z1'])\n",
        "\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "        params['A2'] = self.sigmoid(params['Z2'])\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "        params['A3'] = self.softmax(params['Z3'])\n",
        "\n",
        "        return params['A3']\n",
        "\n",
        "    def backward_pass(self, y_train, output):\n",
        "        '''\n",
        "            Algoritmo de backpropagation\n",
        "            \n",
        "        '''\n",
        "        params = self.params\n",
        "        change_w = {}\n",
        "\n",
        "        # Calculate W3 update\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.sigmoid(params['Z3'], derivative=True)\n",
        "        change_w['W3'] = np.outer(error, params['A2'])\n",
        "\n",
        "        # Calculate W2 update\n",
        "        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)\n",
        "        change_w['W2'] = np.outer(error, params['A1'])\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(params['W2'].T, error) * self.softmax(params['Z1'], derivative=True)\n",
        "        change_w['W1'] = np.outer(error, params['A0'])\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, changes_to_w):\n",
        "        '''\n",
        "            Actualización de los parámetros de la red\n",
        "\n",
        "            θ = θ - η * ∇J(x, y), \n",
        "                theta θ:            a network parameter (e.g. a weight w)\n",
        "                eta η:              the learning rate\n",
        "                gradient ∇J(x, y):  the gradient of the objective function,\n",
        "                                    i.e. the change for a specific theta θ\n",
        "        '''\n",
        "        \n",
        "        for key, value in changes_to_w.items():\n",
        "            self.params[key] -= self.l_rate * value\n",
        "\n",
        "    def compute_accuracy(self, x_val, y_val):\n",
        "        '''\n",
        "            Esta función hace un forward pass con los datos de validación\n",
        "            y comprueba que las etiquetas sean correctas.\n",
        "        '''\n",
        "        predictions = []\n",
        "\n",
        "        for x, y in zip(x_val, y_val):\n",
        "            output = self.forward_pass(x)\n",
        "            pred = np.argmax(output)\n",
        "            predictions.append(pred == np.argmax(y))\n",
        "        \n",
        "        return np.mean(predictions)\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val):\n",
        "        start_time = time.time()\n",
        "        for iteration in range(self.epochs):\n",
        "            for x,y in zip(x_train, y_train):\n",
        "                output = self.forward_pass(x)\n",
        "                changes_to_w = self.backward_pass(y, output)\n",
        "                self.update_network_parameters(changes_to_w)\n",
        "            \n",
        "            accuracy = self.compute_accuracy(x_val, y_val)\n",
        "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "                iteration+1, time.time() - start_time, accuracy * 100\n",
        "            ))"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkYynGc0Byzl"
      },
      "source": [
        "### Results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsg-fGC8Phyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a62329-c338-4c1c-c6ce-93771d943f26"
      },
      "source": [
        "dnn = DeepNeuralNetwork(sizes=[64, 128, 32, 10],epochs=100, l_rate=0.01)\n",
        "dnn.train(X_train, y_train, X_test, y_train)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Time Spent: 0.17s, Accuracy: 13.70%\n",
            "Epoch: 2, Time Spent: 0.34s, Accuracy: 16.70%\n",
            "Epoch: 3, Time Spent: 0.50s, Accuracy: 20.82%\n",
            "Epoch: 4, Time Spent: 0.68s, Accuracy: 23.61%\n",
            "Epoch: 5, Time Spent: 0.85s, Accuracy: 26.17%\n",
            "Epoch: 6, Time Spent: 1.02s, Accuracy: 28.95%\n",
            "Epoch: 7, Time Spent: 1.19s, Accuracy: 32.63%\n",
            "Epoch: 8, Time Spent: 1.36s, Accuracy: 35.41%\n",
            "Epoch: 9, Time Spent: 1.53s, Accuracy: 37.08%\n",
            "Epoch: 10, Time Spent: 1.70s, Accuracy: 38.98%\n",
            "Epoch: 11, Time Spent: 1.87s, Accuracy: 39.87%\n",
            "Epoch: 12, Time Spent: 2.04s, Accuracy: 40.65%\n",
            "Epoch: 13, Time Spent: 2.22s, Accuracy: 42.09%\n",
            "Epoch: 14, Time Spent: 2.39s, Accuracy: 42.76%\n",
            "Epoch: 15, Time Spent: 2.56s, Accuracy: 42.87%\n",
            "Epoch: 16, Time Spent: 2.73s, Accuracy: 43.43%\n",
            "Epoch: 17, Time Spent: 2.90s, Accuracy: 43.99%\n",
            "Epoch: 18, Time Spent: 3.07s, Accuracy: 44.43%\n",
            "Epoch: 19, Time Spent: 3.25s, Accuracy: 44.99%\n",
            "Epoch: 20, Time Spent: 3.41s, Accuracy: 45.43%\n",
            "Epoch: 21, Time Spent: 3.59s, Accuracy: 45.77%\n",
            "Epoch: 22, Time Spent: 3.76s, Accuracy: 45.88%\n",
            "Epoch: 23, Time Spent: 3.93s, Accuracy: 46.21%\n",
            "Epoch: 24, Time Spent: 4.10s, Accuracy: 46.33%\n",
            "Epoch: 25, Time Spent: 4.27s, Accuracy: 46.44%\n",
            "Epoch: 26, Time Spent: 4.44s, Accuracy: 46.66%\n",
            "Epoch: 27, Time Spent: 4.61s, Accuracy: 46.55%\n",
            "Epoch: 28, Time Spent: 4.78s, Accuracy: 46.55%\n",
            "Epoch: 29, Time Spent: 4.95s, Accuracy: 46.55%\n",
            "Epoch: 30, Time Spent: 5.13s, Accuracy: 46.55%\n",
            "Epoch: 31, Time Spent: 5.29s, Accuracy: 46.66%\n",
            "Epoch: 32, Time Spent: 5.46s, Accuracy: 46.77%\n",
            "Epoch: 33, Time Spent: 5.63s, Accuracy: 46.99%\n",
            "Epoch: 34, Time Spent: 5.80s, Accuracy: 47.10%\n",
            "Epoch: 35, Time Spent: 5.97s, Accuracy: 46.99%\n",
            "Epoch: 36, Time Spent: 6.14s, Accuracy: 47.10%\n",
            "Epoch: 37, Time Spent: 6.31s, Accuracy: 46.99%\n",
            "Epoch: 38, Time Spent: 6.48s, Accuracy: 46.99%\n",
            "Epoch: 39, Time Spent: 6.66s, Accuracy: 47.10%\n",
            "Epoch: 40, Time Spent: 6.83s, Accuracy: 47.22%\n",
            "Epoch: 41, Time Spent: 6.99s, Accuracy: 47.33%\n",
            "Epoch: 42, Time Spent: 7.16s, Accuracy: 47.22%\n",
            "Epoch: 43, Time Spent: 7.33s, Accuracy: 47.22%\n",
            "Epoch: 44, Time Spent: 7.50s, Accuracy: 47.33%\n",
            "Epoch: 45, Time Spent: 7.67s, Accuracy: 47.10%\n",
            "Epoch: 46, Time Spent: 7.84s, Accuracy: 47.10%\n",
            "Epoch: 47, Time Spent: 8.02s, Accuracy: 46.99%\n",
            "Epoch: 48, Time Spent: 8.19s, Accuracy: 46.88%\n",
            "Epoch: 49, Time Spent: 8.36s, Accuracy: 46.88%\n",
            "Epoch: 50, Time Spent: 8.53s, Accuracy: 46.66%\n",
            "Epoch: 51, Time Spent: 8.71s, Accuracy: 46.66%\n",
            "Epoch: 52, Time Spent: 8.88s, Accuracy: 46.66%\n",
            "Epoch: 53, Time Spent: 9.06s, Accuracy: 46.77%\n",
            "Epoch: 54, Time Spent: 9.22s, Accuracy: 46.66%\n",
            "Epoch: 55, Time Spent: 9.40s, Accuracy: 46.55%\n",
            "Epoch: 56, Time Spent: 9.56s, Accuracy: 46.66%\n",
            "Epoch: 57, Time Spent: 9.74s, Accuracy: 46.66%\n",
            "Epoch: 58, Time Spent: 9.91s, Accuracy: 46.55%\n",
            "Epoch: 59, Time Spent: 10.08s, Accuracy: 46.55%\n",
            "Epoch: 60, Time Spent: 10.25s, Accuracy: 46.44%\n",
            "Epoch: 61, Time Spent: 10.42s, Accuracy: 46.44%\n",
            "Epoch: 62, Time Spent: 10.59s, Accuracy: 46.44%\n",
            "Epoch: 63, Time Spent: 10.76s, Accuracy: 46.21%\n",
            "Epoch: 64, Time Spent: 10.93s, Accuracy: 45.99%\n",
            "Epoch: 65, Time Spent: 11.11s, Accuracy: 45.88%\n",
            "Epoch: 66, Time Spent: 11.28s, Accuracy: 45.77%\n",
            "Epoch: 67, Time Spent: 11.44s, Accuracy: 45.77%\n",
            "Epoch: 68, Time Spent: 11.61s, Accuracy: 45.55%\n",
            "Epoch: 69, Time Spent: 11.79s, Accuracy: 45.43%\n",
            "Epoch: 70, Time Spent: 11.96s, Accuracy: 45.32%\n",
            "Epoch: 71, Time Spent: 12.13s, Accuracy: 45.21%\n",
            "Epoch: 72, Time Spent: 12.29s, Accuracy: 45.10%\n",
            "Epoch: 73, Time Spent: 12.47s, Accuracy: 44.88%\n",
            "Epoch: 74, Time Spent: 12.64s, Accuracy: 44.77%\n",
            "Epoch: 75, Time Spent: 12.81s, Accuracy: 44.77%\n",
            "Epoch: 76, Time Spent: 12.98s, Accuracy: 44.54%\n",
            "Epoch: 77, Time Spent: 13.15s, Accuracy: 44.43%\n",
            "Epoch: 78, Time Spent: 13.32s, Accuracy: 44.43%\n",
            "Epoch: 79, Time Spent: 13.49s, Accuracy: 44.43%\n",
            "Epoch: 80, Time Spent: 13.66s, Accuracy: 44.32%\n",
            "Epoch: 81, Time Spent: 13.84s, Accuracy: 44.32%\n",
            "Epoch: 82, Time Spent: 14.01s, Accuracy: 44.32%\n",
            "Epoch: 83, Time Spent: 14.18s, Accuracy: 44.21%\n",
            "Epoch: 84, Time Spent: 14.35s, Accuracy: 44.21%\n",
            "Epoch: 85, Time Spent: 14.52s, Accuracy: 44.21%\n",
            "Epoch: 86, Time Spent: 14.69s, Accuracy: 44.10%\n",
            "Epoch: 87, Time Spent: 14.86s, Accuracy: 43.99%\n",
            "Epoch: 88, Time Spent: 15.03s, Accuracy: 43.99%\n",
            "Epoch: 89, Time Spent: 15.20s, Accuracy: 43.76%\n",
            "Epoch: 90, Time Spent: 15.37s, Accuracy: 43.76%\n",
            "Epoch: 91, Time Spent: 15.54s, Accuracy: 43.76%\n",
            "Epoch: 92, Time Spent: 15.71s, Accuracy: 43.76%\n",
            "Epoch: 93, Time Spent: 15.88s, Accuracy: 43.76%\n",
            "Epoch: 94, Time Spent: 16.05s, Accuracy: 43.76%\n",
            "Epoch: 95, Time Spent: 16.22s, Accuracy: 43.88%\n",
            "Epoch: 96, Time Spent: 16.38s, Accuracy: 43.88%\n",
            "Epoch: 97, Time Spent: 16.56s, Accuracy: 43.76%\n",
            "Epoch: 98, Time Spent: 16.73s, Accuracy: 43.76%\n",
            "Epoch: 99, Time Spent: 16.90s, Accuracy: 43.76%\n",
            "Epoch: 100, Time Spent: 17.08s, Accuracy: 43.65%\n"
          ]
        }
      ]
    }
  ]
}

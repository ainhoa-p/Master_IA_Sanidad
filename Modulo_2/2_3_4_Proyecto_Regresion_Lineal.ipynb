{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "2_3_4_Proyecto_Regresion_Lineal.ipynb",
   "provenance": [],
   "authorship_tag": "ABX9TyOWwed9IBNQC/fqwW5gyLor",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_2/2_3_4_Proyecto_Regresion_Lineal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "En este proyecto nuestro objetivo es construir un modelo de regresión lineal simple basado en sci-kit learn para predecir costes médicos a partir del conjunto de datos que puedes encontrar [en este enlace](https://www.kaggle.com/mirichoi0218/insurance))\n",
    "\n",
    "El primer paso será subir a la máquina virtual de Google el archivo de datos"
   ],
   "metadata": {
    "id": "_4ezADhid1nt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cargar librerías básicas y datos"
   ],
   "metadata": {
    "id": "f2TcO1IPUz4E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "AJLvzS0hdxR_",
    "outputId": "6557cda9-886e-45f3-da3d-1d7fb0dc6ccc"
   },
   "source": [
    "# En primer lugar importamos las librerías de trabajo básicas en cualquier\n",
    "# proyecto de aprendizaje automático\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.console import Console\n",
    "console = Console()\n",
    "\n",
    "# Y cargamos los datos contenidos en nuestro dataset (descargado y sub)\n",
    "df = pd.read_csv(\"insurance.csv\")\n",
    "console.log(f\"Cabecera del DataFrame: {df.head()}\", style=\"bold yellow\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploración de datos\n",
    "Como hemos visto en el Tema 2.3.3 uno de los primeros pasos que debemos realizar en todo proyecto de Aprendizaje Automático es la exploración descriptiva y visual de los datos de trabajo. Este proceso es esencial para comprender en profundidad la naturaleza de los datos con los que vamos a trabajar y establecer las bases sobre las cuales se construirán los modelos posteriores.\n"
   ],
   "metadata": {
    "id": "LiUH9FNBgeUJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Observamos cómo se distribuye la variable objetivo \"charges\"\n",
    "# que recoge los cargos realizados al seguro\n",
    "sns.histplot(df['charges'], stat=\"density\")\n",
    "plt.title('Distribución de Cargos de Seguro')\n",
    "plt.xlabel('Cargos')\n",
    "plt.ylabel('Densidad')\n",
    "\n",
    "# Ajustamos los datos a la distribución normal\n",
    "mu, std = norm.fit(df['charges'])\n",
    "\n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2, color='red')\n",
    "plt.legend(['Distribución Normal', 'Datos Observados'])\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n",
    "\n",
    "# Imprimir estadísticas relevantes usando rich.Console\n",
    "console.rule(\"Estadísticas Relevantes\")\n",
    "console.print(f\"Media de Cargos: [bold]{mu:.2f}[/bold]\")\n",
    "console.print(f\"Desviación Estándar de Cargos: [bold]{std:.2f}[/bold]\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "IDHwSelNR3zk",
    "outputId": "94a7ff30-872e-451f-ef5e-097b222b4310"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "El test de Shapiro-Wilk es una prueba estadística utilizada para evaluar si un conjunto de datos sigue una distribución normal. La distribución normal, también conocida como distribución gaussiana, es una distribución continua que es comúnmente encontrada en muchos fenómenos naturales y en estadísticas se considera como una base fundamental para muchos métodos y pruebas."
   ],
   "metadata": {
    "id": "dzSNQDE5R975"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Definamos una función para realizar un test de normalidad y verificar\n",
    "# la gaussianidad (o no) de la distribución\n",
    "\n",
    "from scipy.stats import norm, shapiro\n",
    "\n",
    "def test_normality(data):\n",
    "    stat, p_value = shapiro(data)\n",
    "    console.rule(\"Test de Normalidad\")\n",
    "    console.print(f\"Estadística de Test: [bold]{stat:.4f}[/bold]\")\n",
    "    console.print(f\"P-valor: [bold]{p_value:.4f}[/bold]\")\n",
    "\n",
    "    if p_value > 0.05:\n",
    "        console.print(\"Los datos siguen una distribución normal (p > 0.05).\")\n",
    "    else:\n",
    "        console.print(\"Los datos no siguen una distribución normal (p <= 0.05).\")\n",
    "# Realizar el test de normalidad\n",
    "test_normality(df['charges'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "id": "7OdAy-meRewN",
    "outputId": "0d919b81-63d7-41ed-b865-0e647d529399"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "De los anteriores resultados podemos concluir que la variable objetivo \"charges\" no sigue una distribución normal, sino más bien una distribución mixta, lo que podría ser un problema para obtener un rendimiento óptimo para nuestro modelo lineal"
   ],
   "metadata": {
    "id": "oQYGrsHvhEWN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Revisamos la matriz de correlación para ver posibles dependencias\n",
    "correlation_matrix = df.corr() # Usamos el método implementado por Pandas\n",
    "\n",
    "# Crear una figura y un eje para el heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt=\".2f\")\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.show()\n",
    "\n",
    "# Imprimimos por pantalla la correlación entre características (y \"charges\") ordenando\n",
    "# según mayor valor de correlación\n",
    "console.rule(\"Características Más Correlacionadas con 'charges'\")\n",
    "charges_correlation = correlation_matrix['charges'].sort_values(ascending=False)\n",
    "console.print(charges_correlation)\n",
    "\n",
    "# Otra forma de obtener el mapa de calor empleando Seaborn:\n",
    "# sns.heatmap(df.corr(),annot=True, cmap='viridis')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "Uy5ad-ycRjy9",
    "outputId": "4bd243b7-68c3-4367-86ca-6c31cfa0f964"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Creamos un pair plot para visualizar las morfología de las correlaciones\n",
    "sns.set(style=\"ticks\")\n",
    "pair_plot = sns.pairplot(df)\n",
    "plt.suptitle(\"Pair Plot de las Características\")\n",
    "plt.show()\n",
    "\n",
    "# Imprimir una breve descripción del pair plot\n",
    "console.rule(\"Descripción del Pair Plot\")\n",
    "console.print(\"El pair plot muestra una representación gráfica de las relaciones entre las características del conjunto de datos.\")\n",
    "console.print(\"En la diagonal, se presentan las distribuciones individuales de las características.\")\n",
    "console.print(\"En las celdas fuera de la diagonal, se muestran los scatter plots entre cada par de características, lo que ayuda a identificar patrones de correlación y tendencias.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nQvXyYmOgS5u",
    "outputId": "d824c720-fa99-45f1-a891-3d9ded246ace"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Codificación de variables categóricas\n",
    "El siguiente paso consiste en codificar las variables categóricas"
   ],
   "metadata": {
    "id": "Pf1ojbLzUhcf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Variables categóricas a codificar\n",
    "vars_to_encode = ['sex', 'smoker', 'region']\n",
    "\n",
    "# Utilizamos el método get_dummies para codificar las variables categóricas\n",
    "dummies = [pd.get_dummies(df[var], prefix=var) for var in vars_to_encode]\n",
    "df_dummies = pd.concat(dummies, axis=1)\n",
    "\n",
    "# Eliminamos las columnas originales de las variables categóricas\n",
    "df = df.drop(vars_to_encode, axis=1)\n",
    "\n",
    "# Ahora contatenamos las columnas codificadas con el DataFrame original\n",
    "df_encoded = pd.concat([df, df_dummies], axis=1)\n",
    "\n",
    "# Por último, renombramos las columnas para mayor claridad\n",
    "df_encoded.rename(columns={'smoker_no': 'non-smoker', 'smoker_yes': 'nicotine-user'}, inplace=True)\n",
    "\n",
    "# Mostrar el nuevo dataset correctamente configurado\n",
    "console.log(f\"Dataframe codificado: {df_encoded.head()}\", style=\"bold yellow\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "id": "JmUWDPxsUhI-",
    "outputId": "c5a1accb-3c14-4e2f-c8fa-67aa4616675f"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalización de los datos\n",
    "La siguiente fase del pre-procesado consiste en re-escalar los datos. En este caso aplicaremos métodos de la clase StandardScaler de Sci-kit learn"
   ],
   "metadata": {
    "id": "N2_41g9qVZl0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Crear una copia del DataFrame codificado para el escalado\n",
    "df_c_scaled = df_encoded.copy()\n",
    "\n",
    "# Utilizar StandardScaler para escalar el conjunto de datos\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df_c_scaled)\n",
    "df_c_scaled[df_c_scaled.columns] = scaled_features\n",
    "\n",
    "# Mostrar el conjunto de datos escalado\n",
    "print(\"\\n Dataset escalado: \\n\", df_c_scaled)\n",
    "\n",
    "# Visualizar la distribución de la columna \"charges\" en el conjunto de datos escalado\n",
    "sns.histplot(df_c_scaled[\"charges\"])\n",
    "plt.title(\"Distribución de Cargos Escalados\")\n",
    "plt.xlabel(\"Cargos Escalados\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8R4bXdiqXgDI",
    "outputId": "2c8a7a67-f6b9-4cd4-9845-aee70aa9cc98"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extracción de características"
   ],
   "metadata": {
    "id": "63bgxpkgbjaR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Aplicaremos la técnica PCA para identificar las variables más representativas\n",
    "from sklearn.decomposition import PCA\n",
    "df_s = df_c_scaled # Nuestro DataFrame de trabajo previamente procesado\n",
    "\n",
    "# Los nombres de las características se identifican en las cabeceras de las columnas\n",
    "features = df_s.columns\n",
    "X = df_s[features]\n",
    "\n",
    "# Analizamos el conjunto completo de variables\n",
    "pca = PCA(n_components=len(features), random_state=2020)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# Representamos en un gráfico el procentaje de varianza frente al número de componentes\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_ * 100))\n",
    "plt.xlabel(\"Número de componenetes\")\n",
    "plt.ylabel(\"Porcentaje de varianza explicado\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "vBy1rCtsboId",
    "outputId": "f9636ede-f670-4475-d256-6ec7ea7cd5ad"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Creamos una instancia de PCA con 6 componentes principales\n",
    "pca_s = PCA(n_components=6, random_state=2020)\n",
    "\n",
    "# Ajustamos el modelo PCA a los datos originales\n",
    "pca_s.fit(X)\n",
    "\n",
    "# Transformamos los datos originales utilizando las componentes principales\n",
    "X_pca_s = pca_s.transform(X)\n",
    "\n",
    "# Creamos un nuevo DataFrame con los datos transformados\n",
    "cols = ['PCA' + str(i) for i in range(1, 7)]  # Cambiado para que las componentes sean del 1 al 6\n",
    "df_pca = pd.DataFrame(X_pca_s, columns=cols)\n",
    "\n",
    "# Finalmente observamos el conjunto de datos transformado con las 6 componentes principales\n",
    "console.log(f\"Datos para las 6 componentes principales:{df_pca}\", style=\"bold yellow\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "Et3qTxSceCEq",
    "outputId": "32875428-e6db-4ba5-d18e-4eb1ac1a951c"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ajuste del modelo lineal\n",
    "Completado el proceso de preparación de datos realizaremos el ajuste del modelo"
   ],
   "metadata": {
    "id": "JDkOd6gXbA6N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Las características que vamos a analizar son las seleccionadas con PCA\n",
    "pca_features = df_pca.columns\n",
    "X = df_pca[pca_features]\n",
    "y = df_s['charges']\n",
    "\n",
    "# Usamos la función train_test_split de sklearn para dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "print(\"Tamaño de los conjuntos de entrenamiento y prueba:\")\n",
    "print(\" - X_train:\", X_train.shape)\n",
    "print(\" - X_test:\", X_test.shape)\n",
    "print(\" - y_train:\", y_train.shape)\n",
    "print(\" - y_test:\", y_test.shape)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88765tFybAEt",
    "outputId": "e465f773-6ff1-4abf-ea08-f9f2ac4eb913"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Exponemos el conjunto de entrenamiento al modelo de regresión lineal\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creamos una instancia del modelo de regresión lineal\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Ajustamos el modelo a los datos de entrenamiento\n",
    "lm.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "nYka_jgcBgWu",
    "outputId": "0b4ecd36-5b2c-4235-bcab-d427a3785da7"
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Calculamos las predicciones del modelo sobre el conjunto de prueba\n",
    "y_pred = lm.predict(X_test)\n",
    "\n",
    "# Calculamos descriptores de ajuste y rendimiento del modelo\n",
    "evar = metrics.explained_variance_score(y_test, y_pred)\n",
    "r2 = metrics.r2_score(y_test, y_pred)\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# finalemente, imprimimos por pantall los descriptores de ajuste y rendimiento\n",
    "print(\"Descriptores de ajuste y rendimiento:\")\n",
    "print('- Varianza explicada: ', round(evar, 2))\n",
    "print('- R2:', round(r2, 2))\n",
    "print('- MAE: ', round(mae, 4))\n",
    "print('- MSE: ', round(mse, 4))\n",
    "print('- RMSE: ', round(rmse, 4))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqlOVCEuB476",
    "outputId": "0f02d66e-1444-4cf8-898e-0e80425a9427"
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Para terminar, representamos las predicciones del modelo en una gráfica:\n",
    "\n",
    "# Scatter plot de predicciones vs. valores reales\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(y_pred, y_test, color='blue', edgecolors=(0, 0, 1), label='Datos de prueba')\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3, label='Recta de 45°')\n",
    "\n",
    "# Etiquetas y título del gráfico\n",
    "ax.set_xlabel('Predicción')\n",
    "ax.set_ylabel('Valor real')\n",
    "ax.set_title('Comparación entre Predicciones y Valores Reales')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "BF3-YvBtGMVI",
    "outputId": "16c4e9e6-68da-4c23-b3b3-e1c9e1e11f43"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "k0rOma_tHRJi"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}

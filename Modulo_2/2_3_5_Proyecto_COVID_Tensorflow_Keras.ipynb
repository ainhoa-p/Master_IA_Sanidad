{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "2_3_5_Proyecto_COVID_Tensorflow_Keras.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "CWFHtS42bOUV",
    "vWxzGS08AcVK",
    "hMRiBWpYAiOQ",
    "NMPZ9t8UBogq",
    "UWiQARLdCB9J"
   ],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_2/2_3_5_Proyecto_COVID_Tensorflow_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWFHtS42bOUV"
   },
   "source": [
    "# Clasificación de pacientes COVID-19\n",
    "En este proyecto guiado implementaremos una red neuronal profunda para la clasificación o diagnóstico de pacientes con sospecha de infección por el virus de la COVID-19 a partir de datos de imagen médica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modificar entorno de ejecución y elegir GPU como soporte\n",
    "Antes de comenzar a escribir el código, vamos a modificar el entorno de ejecución de Google Colab para trabajar con hardware GPU. Para ello, vamos al menú de 'Entorno de ejecución' y seleccionamos la opción 'Cambiar tipo de entorno de ejecución' y en el panel que se abrirá a continuación elegimos GPU en el desplegable 'Aceleración por hardware'."
   ],
   "metadata": {
    "id": "deumR1bMLe8f"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMRiBWpYAiOQ"
   },
   "source": [
    "# A) Importación del dataset\n",
    "\n",
    "Cargamos las imágenes médicas después de descargarlas desde [este enlace](https://drive.google.com/file/d/1C6nEoNFr8PmqEddHHYGnXGAWNynz22qm/view?usp=sharing) en el archivo .zip suministrado. Utilizaremos el panel de la izquierda para tenerlas en la máquina virtual que ejecutará este cuaderno.\n",
    "La carpeta de Datos contiene más de dos mil imágenes 2D de radigrafías de toráx en formato jpg. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Opciones para cargar los datos:\n",
    "\n",
    "\n",
    "1.   Si tenéis una cuenta en Google Drive montar la unidad y cargar la carpeta de Datos desde ahí\n",
    "\n"
   ],
   "metadata": {
    "id": "7PEMxN9iWLQR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# El directorio base con la carpeta Datos en Google Drive\n",
    "base_dir = '/content/drive/MyDrive/Documentos/Master IA/Datasets/Datos'"
   ],
   "metadata": {
    "id": "T1_ZGV8IVlYd",
    "outputId": "d78bb5b3-8db1-428d-ceb9-e973d545356d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.   Subir el archivo .zip y descomprimirlo en la MV\n"
   ],
   "metadata": {
    "id": "2VPnD-ifWmwR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip \"/content/Datos.zip\" -d \"/content/Datos/\"\n",
    "base_dir = '/content/datos"
   ],
   "metadata": {
    "id": "e2wmbZj2U3gY",
    "outputId": "1a07cc7a-4069-4818-a7aa-3629b4360e37",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWxzGS08AcVK"
   },
   "source": [
    "# B) Importamos las librerías necesarias\n",
    "Cargamos las librerías Sequential para la configuración de la red formada por capas de convolución y max pooling 2D, capas droptout y capas flatten y dense."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lY1sIb7GI_m5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eee0911a-2e4d-462f-fcb4-86f09e0f1562"
   },
   "source": [
    "from tensorflow.keras.models import Sequential # Modelo/clase sobre la que contruirmos las capas de la red\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "# Conv2D -> para la detección de bordes y definición (nitidez) de la imagen\n",
    "# Pooling -> reduce el tamaño de los datos, la dimensionalidad de la imagen\n",
    "# Dropout -> controla el sobreajuste del modelo\n",
    "# Flatten -> transforma la materiz de caracterísitcas en un vector 1D\n",
    "# Dense conecta el vector de características con el vector de datos de entradas, predicciones de las etiquetas\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Emplearemos Adam como optimizador\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# Nos permite realizar tareas de aumento de datos\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n - A) Importamos las librerías necesarioas\\n\") "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qv3ykJOtAn7k"
   },
   "source": [
    "# Definimos las rutas a las carpetas con las imágenes de entrenamiento y validación\n",
    "import os\n",
    "import os.path as op\n",
    "# Directorio base con las carpetas train y test que contienen las imágenes\n",
    "# que utilizaremos para el entrenamiento y la validación del modelo\n",
    "base_dir = '/content/datasets/Data'\n",
    "train_dir = op.join(base_dir, 'train')\n",
    "test_dir = op.join(base_dir, 'test')\n",
    "\n",
    "# Dentro de cada conjunto tenemos dos directorios con radiografías de tórax\n",
    "# para cada grupo: COVID o Normal\n",
    "train_covid_dir = op.join(train_dir, 'COVID19')\n",
    "train_normal_dir = op.join(train_dir, 'NORMAL')\n",
    "test_covid_dir = op.join(test_dir, 'COVID19')\n",
    "test_normal_dir = op.join(test_dir, 'NORMAL')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s_QyIqfcBUwv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4b207975-95c3-41fc-bdec-31fd973a0a07"
   },
   "source": [
    "# Echamos un vistazo a las imágenes\n",
    "# Conjunto de entrenamiento\n",
    "train_covid_names = sorted(os.listdir(train_covid_dir))\n",
    "print(\"\\n - Primeras 10 imágenes de entrenamiento (COVID):\", train_covid_names[0:10])\n",
    "train_normal_names = sorted(os.listdir(train_normal_dir))\n",
    "print(\"\\n - Primeras 10 imágenes de entrenamiento (NORMAL):\", train_normal_names[0:10])\n",
    "# Conjunto de evaluación\n",
    "test_covid_names = sorted(os.listdir(test_covid_dir))\n",
    "print(\"\\n - Primeras 10 imágenes de evaluación (COVID):\", test_covid_names[0:10])\n",
    "test_normal_names = sorted(os.listdir(test_normal_dir))\n",
    "print(\"\\n - Primeras 10 imágenes de evaluación (NORMAL):\", test_normal_names[0:10])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8_PDJQWuBU8U",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dcae2f14-8019-4499-cb81-5417b4cc99d3"
   },
   "source": [
    "# Cuántas imágenes tenemos en nuestros conjuntos de datos\n",
    "print(\" => Imágenes en el conjunto de entrenamiento:\", len(train_covid_names+train_normal_names))\n",
    "print(\" => Imágenes en el conjunto de evaluación:\", len(test_covid_names+test_normal_names))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMPZ9t8UBogq"
   },
   "source": [
    "# C) Visualización de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vryI0-PKBtLF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "outputId": "547af0f4-8cd7-4d3a-f3b7-8dcb365a2f74"
   },
   "source": [
    "# Vamos a visualizar algunas imágenes del conjunto de datos en una cuadrícula 4x4\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "fig = plt.figure(1, figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(4, 4, figure=fig)\n",
    "covid_pics = [op.join(train_covid_dir, filename) for filename in train_covid_names[0:8]]\n",
    "normal_pics = [op.join(train_normal_dir, filename) for filename in train_normal_names[0:8]]\n",
    "merger_pics = covid_pics + normal_pics\n",
    "for i, pic_path in enumerate(merger_pics):\n",
    "  pic_name = op.basename(merger_pics[i])\n",
    "  ax = fig.add_subplot(gs[i])\n",
    "  pic_data = mpimg.imread(pic_path)\n",
    "  ax.imshow(pic_data, cmap='gray')\n",
    "  ax.set_title(pic_name)\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWiQARLdCB9J"
   },
   "source": [
    "# D) Pre-procesado y Aumento de datos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZU7C28QWB_il",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "121ea3a2-49e7-4d44-be5d-b25c04c14f52"
   },
   "source": [
    "# Generamos los lotes de entrenamiento, evaluación y validación\n",
    "dgen_train = ImageDataGenerator(rescale=1./255,\n",
    "                                validation_split=0.2,\n",
    "                                zoom_range = 0.2,\n",
    "                                horizontal_flip = True)\n",
    "dgen_test = ImageDataGenerator(rescale=1./255)\n",
    "dgen_val = ImageDataGenerator(rescale=1./255)\n",
    "# Sobre cada grupo aplicamos una renormalización para que los datos estén\n",
    "# en el intervalo 0-1 (el valor máximo de intensidad es 255)\n",
    "# Sobre el conjunto de entrenamiento aplicamos métodos de aumento de datos\n",
    "# zoom_range -> porcentaje máximo de zoom sobre la imagen\n",
    "# horizontal_flip -> aplicar desplazamiento horizontal \n",
    "\n",
    "# Creamos los generadores de lotes conteniendo el 80% de las imágenes en el\n",
    "# grupo de entrenamiento y el 20% restante en el de validación\n",
    "train_generator = dgen_train.flow_from_directory(train_dir,\n",
    "                                           target_size=(150, 150),\n",
    "                                           subset = 'training',\n",
    "                                           batch_size = 32,\n",
    "                                           class_mode = 'binary')\n",
    "# Target size -> redimensiona las imágenes para que se acomoden en un tamaño de 150x150 píxeles\n",
    "# Mode -> especifica que estos datos se utilizarán en la fase de entrenamiento\n",
    "# batch_size -> número de imágenes que cargaremos en memoria en cada paso del proceso de aprendizaje\n",
    "# class_mode -> binary para clasificación dicotómica (COVID / Normal), categorical para identificar etiquetas\n",
    "\n",
    "val_generator = dgen_train.flow_from_directory(train_dir,\n",
    "                                           target_size=(150, 150),\n",
    "                                           subset = 'validation',\n",
    "                                           batch_size = 32,\n",
    "                                           class_mode = 'binary')\n",
    "\n",
    "test_generator = dgen_train.flow_from_directory(test_dir,\n",
    "                                           target_size=(150, 150),\n",
    "                                           batch_size = 32,\n",
    "                                           class_mode = 'binary')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SIdVxgoLGb0K",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "22a9c303-155a-44f1-99ba-ce674bae5654"
   },
   "source": [
    "# Como sabemos, las dos clases de nuestro problema son COVID y Normal, veámoslo:\n",
    "train_generator.class_indices"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gMe4b6_YGdlt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d0926c92-4d1a-4acf-b766-8a2f61b3d04e"
   },
   "source": [
    "# Veámos también cuál es el tamaño de la muestra para el entrenamiento del modelo\n",
    "train_generator.image_shape\n",
    "# El tamaño de las imágenes es 150x150 y el valor de dimensión tres nos indica que las imágenes\n",
    "# están en formato RGB, donde el color de cada pixel es una combinación de rojo+verde+azul"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCRKSE8I0gop"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELcsfIibGoK3"
   },
   "source": [
    "# E) Construcción de la Red Neuronal Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_-pwXOyuGxIq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5bfd4ecc-a247-4551-c055-ee656579c5c2"
   },
   "source": [
    "# En este punto ya podemos definir nuestro modelo de RN convolucional que aprenderá\n",
    "# de los datos agrupados que hemos trabajado anteriorment\n",
    "# Construiremos el modelo añadiendo capas a una instancia de la clase Sequencial\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# La primera capa que utilizaremos para extraer caracterísiticas de la imagen es\n",
    "# una capa de convolución que aplica filtros formados por pequeños cuadrados que \n",
    "# mapean la imagen de entrada. Seleccionaremos 32 características a extraer\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), padding='SAME', activation='relu', input_shape=(150, 150, 3))) \n",
    "# Añadimos la capa Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Capa Dropout que \n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Vamos a añadir una segunda capa de convolución\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), padding='SAME', activation='relu'))\n",
    "# Y las sucesivas capas de Pooling y Dropout \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Ahora podemos añadir la capa Flatten \n",
    "model.add(Flatten())\n",
    "\n",
    "# La capa densamente conexa en la que especificamos los nodos y el\n",
    "# tipo de activiación\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# Añadirmos una nueva operación Dropout que reduce a la mitad el número de nodos\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Y finalemente conectamos los nodos para crear la salida con un único nodo\n",
    "# Dado que estamos ante un problema de clasificación usaremos la función de activación\n",
    "# sigmoidea\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# Resumen del modelo\n",
    "print(\"Resumen del modelo:\\n\", model.summary())\n",
    "# En este resumen vemos que el tamaño del primer tensor tiene valor None porque \n",
    "# se refiere a la dimensionalidad del tamaño de lote, que puede tomar cualquier valor\n",
    "# que mejor modelo produzca\n",
    "\n",
    "# Tras la aplicación de la capa de convolución tenemos un tensor de dimensión igual a \n",
    "# la mitad de la imagen de entrada y con 32 características en el eje z"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgCag-_HHG2N"
   },
   "source": [
    "# F) Compilar y Entrenar el Modelo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1iVcxB1ZHNDV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aa5d24af-afd0-4784-d6a4-b6e548a43e36"
   },
   "source": [
    "# Para compilar nuestro modelo de RNC tendremos que:\n",
    "# 1) Definir el método de optimización (Adam)\n",
    "# 2) El valor para el learning_rate\n",
    "# 3) La función de pérdidad: bynary cross entropy es una buena elección para una tarea de clasificación binaria\n",
    "# 4) La métrica de evaluación: utilizaremos accuracy\n",
    "model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OWtZjCFlHPQs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a4ca67f0-8150-4cf8-ca62-805a7d4b077c"
   },
   "source": [
    "# Compilado el modelo podemos lanzar el proceso de entrenamiento\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=30,\n",
    "                    validation_data=val_generator)\n",
    "# El objeto history registra el progreso durante el entrenamiento, recoge el valor de la función de pérdida\n",
    "# y la métrica de evaluación en cada paso del proceso "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z95l8IC5HRoG"
   },
   "source": [
    "# G) Evaluación del desempeño"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M0d8G-BEHXXs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e74135ab-edff-4d92-b283-8fcc39d53152"
   },
   "source": [
    "# Veámos que los valores de loss y accuracy se han almacenado en history\n",
    "history.history.keys()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EVtqI0QuHbZR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "outputId": "f4d63060-5472-4bd9-9ec3-d83a11a340ac"
   },
   "source": [
    "# Para evaluar el rendimiento del modelo representamos los valores de las métricas de interés en función de la\n",
    "# época del entrenamiento\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss (entrenamiento)', 'loss (validación'])\n",
    "plt.title(\"Evolución del valor de la función de pérdida\")\n",
    "plt.xlabel('época')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CEIibDQOHgfT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "outputId": "be761c5c-b943-486f-e1a9-d935ccf7bbd5"
   },
   "source": [
    "# Un vistazo a la evolución de la precisión\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['Precisión (entrenamiento)', 'Precisión (validación)'])\n",
    "plt.title(\"Evolución del valor de precisión\")\n",
    "plt.xlabel('época')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U9Zm__wJHm3v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "82524dc7-baa2-4d11-c2dd-2779434a1e07"
   },
   "source": [
    "# Por último, validamos el modelo sobre la muestra de evaluación\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "# Simplemente tenemos que evaluar el modelo que acabamos de entrenar sobre el conjunto de datos que habíamos\n",
    "# reservado para la evaluación \n",
    "print(\"\\n=> Resultados sobre el conjunto de pruebas:\")\n",
    "print(\" - Loss: {:.2f}, Accuracy: {:.2f} \".format(test_loss, test_accuracy))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AZjx1Q2HseZ"
   },
   "source": [
    "# H) Resultados (predicciones) sobre datos no vistos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b5qflf3jH3o_",
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
       "ok": true,
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "status": 200,
       "status_text": ""
      }
     },
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "outputId": "f07ad29c-5f8e-45e2-add9-ca9bb2a38240"
   },
   "source": [
    "# En esta última acción evaluaremos nuestro modelo sobre todo el conjunto de imágenes de radiografías torácicas\n",
    "# para obtener un resultado (predicción) para el paciente: infección COVID o sano\n",
    "from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "uploads = files.upload()\n",
    "for filename in uploads.keys():\n",
    "  img_path = '/content/' + filename\n",
    "  img = image.load_img(img_path, target_size=(150, 150))\n",
    "  data = image.img_to_array(img)\n",
    "  data = np.expand_dims(data, axis=0)\n",
    "  prediction = model.predict(data)\n",
    "  print(\"\\nImagen Rayos-X:\", filename)\n",
    "\n",
    "  if prediction == 0:\n",
    "    print(\" => Detectado COVID-19\")\n",
    "  else:\n",
    "    print(\" => Estado normal\")\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}

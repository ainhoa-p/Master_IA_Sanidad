{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_3/Clases/Semana_26_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH0raAsM5TSa"
      },
      "source": [
        "Este cuaderno es una adaptación del trabajo previo de:\n",
        "https://github.com/matiasbattocchia/datitos\n",
        "\n",
        "\n",
        "Vamos a hacer un recorrido por los pasos básicos del pre-procesamiento de texto. Estos pasos son necesarios para transformar texto del lenguaje humano a un formato legible para máquinas para su posterior procesamiento.\n",
        "\n",
        "Veremos cómo realizar estos pasos con código propio, para mayor entendimiento de lo que está sucediendo, y con [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsgRit3B5TSb"
      },
      "source": [
        "En concreto, los pasos son:\n",
        "\n",
        "1. **Limpieza**, la remoción del contenido no deseado.\n",
        "2. **Normalización**, la conversión diferentes formas a una sola. \n",
        "3. **Tokenización**, la separación del texto en tókenes (unidades mínimas, por ejemplo palabras).\n",
        "4. Separación en **conjuntos de datos**: entrenamiento, validación, prueba.\n",
        "5. Generación del **vocabulario**, la lista de tókenes conocidos.\n",
        "6. **Numericalización**, el mapeo de tókenes a números enteros.\n",
        "\n",
        "Nota: El órden de los primeros tres pasos (limpieza, normalización, tokenización) puede variar según conveniencia. El resto de los pasos mantiene el órden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaVz_Tun5TSc"
      },
      "source": [
        "## Dataset de ejemplo\n",
        "\n",
        "¿Qué sería de esta publicación sin algunos ejemplos? En nuestro caso vamos a utilizar el dataset CodiEsp. Se trata de un corpus de textos clínicos en español:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/sample_data\n",
        "!wget https://zenodo.org/record/3837305/files/codiesp.zip\n",
        "!unzip codiesp.zip >> /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nb7tDJoApCV",
        "outputId": "dceabe0c-f791-490d-946f-75b3bc6be8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/sample_data': No such file or directory\n",
            "--2023-02-13 17:48:58--  https://zenodo.org/record/3837305/files/codiesp.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11239591 (11M) [application/octet-stream]\n",
            "Saving to: ‘codiesp.zip.1’\n",
            "\n",
            "codiesp.zip.1       100%[===================>]  10.72M   521KB/s    in 21s     \n",
            "\n",
            "2023-02-13 17:49:20 (522 KB/s) - ‘codiesp.zip.1’ saved [11239591/11239591]\n",
            "\n",
            "replace final_dataset_v4_to_publish/README.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8CNvfcd5TSd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from os.path import join\n",
        "\n",
        "dir_dataset = 'final_dataset_v4_to_publish'\n",
        "train_dataset = join(dir_dataset,'train','trainX.tsv')\n",
        "\n",
        "df = pd.read_csv(train_dataset, sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7pFpRdS5TSe",
        "outputId": "0c8f51dc-88ff-4ce4-df4a-aab8549d5a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                              0            1       2  \\\n",
              "1989  S0211-57352007000200017-1  DIAGNOSTICO  r41.0    \n",
              "8657  S1698-69462006000400005-1  DIAGNOSTICO  c64.1    \n",
              "7264  S1134-80462015000100006-1  DIAGNOSTICO  r52      \n",
              "6820  S1130-63432014000200011-1  DIAGNOSTICO  i10      \n",
              "2553  S0211-69952015000200015-1  DIAGNOSTICO  c94.6    \n",
              "8892  S1699-695X2015000300013-1  DIAGNOSTICO  e05.90   \n",
              "658   S0004-06142009000100010-2  DIAGNOSTICO  r53.1    \n",
              "8251  S1139-76322015000100013-1  DIAGNOSTICO  l29.9    \n",
              "8660  S1698-69462006000400005-1  DIAGNOSTICO  k63.5    \n",
              "8409  S1139-76322016000200010-1  DIAGNOSTICO  r60.9    \n",
              "\n",
              "                                 3                    4  \n",
              "1989  delirio                       2271 2278            \n",
              "8657  derecha carcinoma renal       2811 2818;2925 2940  \n",
              "7264  dolor                         1655 1660            \n",
              "6820  hipertensión                  211 223              \n",
              "2553  procesos mieloproliferativos  1269 1297            \n",
              "8892  hipertiroidismo               2040 2055            \n",
              "658   astenia                       655 662              \n",
              "8251  pruriginosas                  105 117              \n",
              "8660  pólipos colónica              1992 1999;2033 2041  \n",
              "8409  edema                         251 256              "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47a7f99f-63b5-4bf6-bdd8-a8058728180b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1989</th>\n",
              "      <td>S0211-57352007000200017-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r41.0</td>\n",
              "      <td>delirio</td>\n",
              "      <td>2271 2278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8657</th>\n",
              "      <td>S1698-69462006000400005-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>c64.1</td>\n",
              "      <td>derecha carcinoma renal</td>\n",
              "      <td>2811 2818;2925 2940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7264</th>\n",
              "      <td>S1134-80462015000100006-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r52</td>\n",
              "      <td>dolor</td>\n",
              "      <td>1655 1660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6820</th>\n",
              "      <td>S1130-63432014000200011-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>i10</td>\n",
              "      <td>hipertensión</td>\n",
              "      <td>211 223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2553</th>\n",
              "      <td>S0211-69952015000200015-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>c94.6</td>\n",
              "      <td>procesos mieloproliferativos</td>\n",
              "      <td>1269 1297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8892</th>\n",
              "      <td>S1699-695X2015000300013-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>e05.90</td>\n",
              "      <td>hipertiroidismo</td>\n",
              "      <td>2040 2055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>S0004-06142009000100010-2</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r53.1</td>\n",
              "      <td>astenia</td>\n",
              "      <td>655 662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8251</th>\n",
              "      <td>S1139-76322015000100013-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>l29.9</td>\n",
              "      <td>pruriginosas</td>\n",
              "      <td>105 117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8660</th>\n",
              "      <td>S1698-69462006000400005-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>k63.5</td>\n",
              "      <td>pólipos colónica</td>\n",
              "      <td>1992 1999;2033 2041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8409</th>\n",
              "      <td>S1139-76322016000200010-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r60.9</td>\n",
              "      <td>edema</td>\n",
              "      <td>251 256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47a7f99f-63b5-4bf6-bdd8-a8058728180b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47a7f99f-63b5-4bf6-bdd8-a8058728180b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47a7f99f-63b5-4bf6-bdd8-a8058728180b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "with pd.option_context('display.max_colwidth', -1):\n",
        "    \n",
        "    display(df.sample(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text_file = join(dir_dataset,'train','text_files','S0211-69952015000200015-1.txt')\n",
        "f = open(example_text_file,'r')\n",
        "example_text=f.read()\n",
        "print(example_text)\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "6KrKOUM2Bd31",
        "outputId": "157d56f6-43e0-4976-8ea9-8a57c08178ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Varón de 70 años de raza blanca, con antecedentes de hiperplasia prostática benigna, fibrilación auricular recurrente y hemitiroidectomía derecha por hiperplasia nodular, así como insuficiencia renal crónica no estudiada (creatinina basal 1,5 mg/dl).\n",
            "Acude a Urgencias por clínica de tres meses de evolución de inestabilidad de la marcha y debilidad generalizada (ingreso previo en otro centro por estos síntomas). Al examen físico, el paciente está deshidratado, confuso, bradipsíquico y con temblor distal.\n",
            "Los datos analíticos se resumen en la (tabla 1), destacando deterioro importante de la función renal, hipercalcemia severa y elevación marcada de PTH (veinte veces sobre el valor de referencia del laboratorio). Se inicia sueroterapia, perfusión de furosemida y corticoides endovenosos, bifosfonatos y calcitonina. Debido a la severidad de la clínica neurológica, se indica simultáneamente terapia renal sustitutiva urgente mediante hemodiálisis con baja concentración de calcio en el dializado. Presenta mejoría clínica transitoria, pero debido a persistencia del fracaso renal y a rebote de la hipercalcemia (aumento de calcemia > 2 mg/dl 24 horas post-hemodiálisis), requiere continuar con sesiones diarias.\n",
            " \n",
            "Como parte del estudio etiológico, se descartan procesos mieloproliferativos y otras neoplasias (no componente monoclonal en proteinograma, proteinuria de Bence-Jones negativa), patología infecciosa (serologías negativas) y enfermedades autoinmunes. Ecografía urinaria con riñones de tamaño normal y adecuada diferenciación cortico-medular. La TAC toraco-abdominal informa de masa retroesofágica de 5,6 × 3,2 × 6,8 cm, sospechosa de tumor paratiroideo, y la imagen se confirma mediante gammagrafía paratiroidea, además de nódulos hipodensos suprarrenales bilaterales: derecho 27 × 18 mm e izquierdo de 15 × 11 mm.\n",
            "Con estos hallazgos se consulta con Endocrinología que descarta feocromocitoma (catecolaminas y metanefrinas en orina negativas). En sesión conjunta con Cirugía General se decide manejo quirúrgico, resecando tumor paratiroideo de 8 cm. La Anatomía Patológica informa de adenoma paratiroideo sin invasión capsular o vascular.\n",
            " \n",
            "En el post-operatorio inmediato se produce \"síndrome de hueso hambriento\", por lo que el paciente requiere suplemento endovenoso y oral de calcio por hipocalcemia severa, y sesiones interdiarias de hemodiálisis hasta nueve días después de la cirugía.\n",
            "Nueve meses después, el paciente está asintomático, con calcemia en rango sin necesidad de suplementos, y con recuperación parcial de la función renal (creatinina 2,7 mg/dl). Es de destacar que la función renal persiste alterada un año tras la cirugía, a pesar de la normalización de la calcemia.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-yzBMbj5TSf"
      },
      "source": [
        "## Expresiones regulares\n",
        "\n",
        "Si las expresiones regulares no te resultan familiares entonces vale la pena estudiarlas brevemente, ya que las usaremos. Podés mirar este [tutorial](https://robologs.net/2019/05/05/como-utilizar-expresiones-regulares-regex-en-python) que encontramos en la web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz7O8kr95TSf"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxN89Odx5TSg"
      },
      "source": [
        "## Limpieza\n",
        "\n",
        "Muchas técnicas modernas no realizan limpieza alguna. Dependiendo de lo que queramos hacer tal vez convenga deshacernos de algunos elementos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG0f8Tcn5TSg"
      },
      "outputs": [],
      "source": [
        "def limpiar(texto):\n",
        "    puntuación = r'[,;.:¡!¿?@#$%&[\\](){}<>~=+\\-*/|\\\\_^`\"\\']'\n",
        "    \n",
        "    # signos de puntuación\n",
        "    texto = re.sub(puntuación, ' ', texto)\n",
        "\n",
        "    return texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKHcP8GJ5TSh"
      },
      "source": [
        "En esta función substituimos los signos de puntuación\n",
        "    \n",
        "    , ; . : ¡ ! ¿ ? @ # $ % & [ ] ( ) { } < > ~ = + - * / | \\ _ ^ ` \" '\n",
        "\n",
        "por espacios (me gusta más; usar string vacío `''` para eliminarlos) medieante expresiones regulares (algunos caracteres tuvieron que ser escapados anteponiendo `\\` por tener un significado especial para la expresión regular). Hacemos lo mismo con los dígitos. Veamos un ejemplo de funcionamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "e0CIM4rO5TSi",
        "outputId": "e87607b1-5797-4318-a28b-c3198b15db07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'El paciente presenta fiebre febrícula  tos y mocos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 222
        }
      ],
      "source": [
        "limpiar('El paciente presenta fiebre/febrícula, tos y mocos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEip6nU65TSi"
      },
      "source": [
        "Otros elementos que podríamos pensar en remover son caracteres invisibles, espacios redundantes. Veremos que esto en particular también puede ser resulto en la tokenización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7dLdL05TSj"
      },
      "source": [
        "## Normalización\n",
        "\n",
        "Normalizar es la tarea de llevar lo que puede ser expresado de múltiples maneras como fechas, números y abreviaturas a una única forma. Por ejemplo\n",
        "\n",
        "     13/03/30 -> trece de marzo de dos mil treinta\n",
        "     DC -> departamento de computación\n",
        "\n",
        "Se trata de una práctica clásica de la época de los modelos de lenguaje probabilísticos, que intentaban reducir lo más posible la cantidad de palabras. En cierta forma 1 palabra = 1 atributo (lo que en los '90s conocimos como convertibilidad). Elegir atributos es ingeniería de atributos, la parte central del *machine learning*, y lo justamente lo que el *deep learning* busca automatizar.\n",
        "\n",
        "Sin embargo hay una normalización muy común hoy, el **convertir todo el texto a minúsculas**. En el caso del español, una normalización común es la **remoción de tildes**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JT_E6ap5TSj"
      },
      "outputs": [],
      "source": [
        "def normalizar(texto):\n",
        "    # todo a minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # tildes y diacríticas\n",
        "    texto = re.sub('á', 'a', texto)\n",
        "    texto = re.sub('é', 'e', texto)\n",
        "    texto = re.sub('í', 'i', texto)\n",
        "    texto = re.sub('ó', 'o', texto)\n",
        "    texto = re.sub('ú', 'u', texto)\n",
        "    texto = re.sub('ü', 'u', texto)\n",
        "    texto = re.sub('ñ', 'n', texto)\n",
        "\n",
        "    return texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "SkFF96gW5TSk",
        "outputId": "d95a4f86-b99b-43c9-8951-1df613bddc90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'el paciente es espanol y tomara antibioticos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 223
        }
      ],
      "source": [
        "normalizar('El paciente es español y tomará antibióticos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYm73iir5TSl"
      },
      "source": [
        "Hay una librería llamada [unidecode](https://pypi.org/project/Unidecode) que realiza transliteración: representa letras o palabras de un alfabeto en otro, útil si tenemos caracteres en ruso (cirílico) o chino (caracteres Han), aún útil para el alfabeto latino cuando queremos pasar de Unicode a ASCII (lo que substituiría las tildes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "6rSyEYR05TSl",
        "outputId": "b2182f76-64d9-41f7-a1f4-91f5e57ef46a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.8/dist-packages (1.3.6)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'El paciente es espanol y tomara antibioticos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 224
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "\n",
        "unidecode('El paciente es español y tomará antibióticos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjZ365Jv5TSm"
      },
      "source": [
        "Una normalización que no vale la pena intentar con este dataset es la **correción ortográfica** con un paquete como [pyspellchecker](https://pypi.org/project/pyspellchecker). Los artículos incluídos en codiesp ya cuentan con corrección ortográfica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBjydBHV5TSm"
      },
      "source": [
        "## Tokenización\n",
        "\n",
        "Tokenizar es separar el texto en partes más pequeñas llamadas tókenes. Una unidad muy común es la palabras pero depende de lo que queramos hacer, si es que no hemos eliminado a los signos de puntuación estos también serían tókenes. Las palabras frecuentemente están compuestas por una raíz, prefijo y/o sufijo, por lo que podríamos decidir separarlos también. En inglés es común separar `it's` en `it` y `'s`, si bien en español esta situación no es común.\n",
        "\n",
        "A diferencia de la limpieza y la normalización, la tokenización **es un paso indispesable** en la preparación de texto para su procesamiento.\n",
        "\n",
        "Para el dataset en cuestión la tokenización es simple, vamos a separar seǵun espacios y demás caracteres invisibles como `\\t` (tabulación) y `\\n` (salto de línea). De haber signos de puntuación, pro ejemplo si quisiéramos procesar un documento extenso en oraciones, el proceso es más complejo ya que `final.` tiene un punto en vez de un espacio, y no siempre los puntos demarcan el final de un tóken como en `A.M.` y `P.M.`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR7vfvrv5TSn"
      },
      "source": [
        "Debemos definir si elementos como los signos de puntuación son tókenes o si simplemente delimitan palabras o tókenes, en cuyo caso desaparecerían en el proceso. Mismo con los caracteres invisibles, si estuviésemos haciendo un modelo que programe en Python, la indentación es fundamental y deberiera mantenerse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGiDceSb5TSn"
      },
      "outputs": [],
      "source": [
        "def tokenizar(texto):\n",
        "    # IMPORTANTE: podría devolver una lista vacía\n",
        "    return [tóken for tóken in texto.split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjzVVT5Y5TSo"
      },
      "source": [
        "`split` también se encarga de los caracteres invisibles repetidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "NNgFCDas5TSo",
        "outputId": "d04654bd-8256-4c18-af15-10506e0a481c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['El', 'paciente', 'presenta', 'síntomas', 'de', 'gripe']"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ],
      "source": [
        "tokenizar('El paciente presenta síntomas de gripe')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6lCqs-b5TSq"
      },
      "source": [
        "Varios modelos de lenguaje utilizan caracteres en vez de palabras como tókenes, esto es útil por varios motivos que listaremos más adelante. Otros utilizan partes de palabras como sílabas (las partes se determinan estadísticamente). Ver https://arxiv.org/pdf/1508.07909.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj4SLAtg5TSr"
      },
      "source": [
        "### Tokenización utilizando alguna librería\n",
        "\n",
        "    pip install spacy\n",
        "    python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34ZPafQXsNkS",
        "outputId": "a1f87405-a8cc-40da-94a3-e5c7f49a2006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-02-13 17:52:23.719526: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-13 17:52:23.719731: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-13 17:52:23.719760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-13 17:52:27.272289: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from es-core-news-sm==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.25.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.12)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.10.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "jQunLCt35TSr",
        "outputId": "6684803d-60f9-4ebb-ffdf-189e2c4f2cba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El', 'paciente', 'tiene', 'síntomas', 'de', 'gripe']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "doc = nlp('El paciente tiene síntomas de gripe')\n",
        "\n",
        "print([tóken.text for tóken in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bn-1Fuu5TS1"
      },
      "source": [
        "### *Stop words*\n",
        "\n",
        "Hay listas armadas de palabras muy comunes (*stop words*). Podemos elaborarla de alguna manera o usar alguna existente.\n",
        "\n",
        "    pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF7aZGVDx9ml",
        "outputId": "f3287e28-e7d8-4c54-c07d-6ce3b8e5da3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "N7TkUzpw5TS1",
        "outputId": "a32a8dfa-d962-45de-e4d2-238aab816d62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "    \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords.words('spanish') [0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJVoIDuL5TS1"
      },
      "source": [
        "Un detalle a cuidar es que la tokenización usada para la lista de *stop words* tiene que haber sido la misma o similar que la usada para los documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "B0PuDtWk5TS1",
        "outputId": "acb99c10-ec37-4b7b-94a1-bd6d025736fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['paciente', 'síntomas', 'gripe']"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ],
      "source": [
        "def filtrar_stop_words(doc):\n",
        "    return [tóken for tóken in doc if tóken not in stopwords.words('spanish')]\n",
        "\n",
        "filtrar_stop_words(\n",
        "    ['el', 'paciente', 'tiene', 'síntomas', 'de', 'gripe'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdJhCCNs5TSr"
      },
      "source": [
        "## Otros pre-procesos\n",
        "\n",
        "Clásicamente se aplicaban alguno de estos para reducir aún más la cantidad de palabras:\n",
        "\n",
        "#### *Stemming*\n",
        "\n",
        "*Stem*, de raíz, reduce la inflección de las palabras, mapeando un grupo de palabras a la misma raíz, sin importar si la raíz es una palabras válida en el lenguaje.\n",
        "\n",
        "     caminando, caminar, camino -> camin\n",
        "\n",
        "#### *Lemmatization*\n",
        "\n",
        "A diferencia del *stemming*, la lematización reduce las palabras inflexadas a palabras que pertenecen al lenguaje. La raíz pasa a llamarse *lema*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSfSPyC_5TSr"
      },
      "source": [
        "## Primera parte del pre-procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "vHPYRiat5TSr"
      },
      "outputs": [],
      "source": [
        "def preprocesar(texto):\n",
        "    texto = limpiar(texto)\n",
        "    texto = normalizar(texto)\n",
        "    texto = tokenizar(texto)\n",
        "    texto = filtrar_stop_words(texto)\n",
        "\n",
        "    return texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXct2b4K5TSr"
      },
      "source": [
        "## Conjuntos de datos\n",
        "\n",
        "En la competencias normalmente encontramos dos archivos, el de entrenamiento y el de inferencia —que le suelen llamar de prueba y es el que tenemos que predecir para entregar—. Del que suelen llamar `train` también tenemos que obtener el de validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "2CIh7cXX5TSs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "78ff5a39-951b-4275-dc44-502ad5e1a373"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              0            1        2  \\\n",
              "2     S0004-06142005000700014-1  DIAGNOSTICO    n44.8   \n",
              "3     S0004-06142005000700014-1  DIAGNOSTICO  z20.818   \n",
              "4     S0004-06142005000700014-1  DIAGNOSTICO    r60.9   \n",
              "5     S0004-06142005000700014-1  DIAGNOSTICO      r52   \n",
              "6     S0004-06142005000700014-1  DIAGNOSTICO    a23.9   \n",
              "...                         ...          ...      ...   \n",
              "9176  S2340-98942015000100005-1  DIAGNOSTICO   r06.00   \n",
              "9177  S2340-98942015000100005-1  DIAGNOSTICO    c56.2   \n",
              "9178  S2340-98942015000100005-1  DIAGNOSTICO    r97.1   \n",
              "9179  S2340-98942015000100005-1  DIAGNOSTICO      r55   \n",
              "9180  S2340-98942015000100005-1  DIAGNOSTICO   r06.00   \n",
              "\n",
              "                                                 3          4  \n",
              "2                teste derecho aumentado de tamaño  1343 1376  \n",
              "3                            exposición a Brucella    594 615  \n",
              "4                                           edemas  1250 1256  \n",
              "5                                          dolores      78 85  \n",
              "6                                         Brucella    607 615  \n",
              "...                                            ...        ...  \n",
              "9176                                        disnea    942 948  \n",
              "9177  carcinoma seroso papilar en ovario izquierdo     94 138  \n",
              "9178                           elevación de CA 125    413 432  \n",
              "9179                       pérdida de conocimiento    959 982  \n",
              "9180                                        disnea  1542 1548  \n",
              "\n",
              "[7209 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81fb1c2b-d018-4658-accd-f14161c75fca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S0004-06142005000700014-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>n44.8</td>\n",
              "      <td>teste derecho aumentado de tamaño</td>\n",
              "      <td>1343 1376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>S0004-06142005000700014-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>z20.818</td>\n",
              "      <td>exposición a Brucella</td>\n",
              "      <td>594 615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>S0004-06142005000700014-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r60.9</td>\n",
              "      <td>edemas</td>\n",
              "      <td>1250 1256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>S0004-06142005000700014-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r52</td>\n",
              "      <td>dolores</td>\n",
              "      <td>78 85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>S0004-06142005000700014-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>a23.9</td>\n",
              "      <td>Brucella</td>\n",
              "      <td>607 615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9176</th>\n",
              "      <td>S2340-98942015000100005-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r06.00</td>\n",
              "      <td>disnea</td>\n",
              "      <td>942 948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9177</th>\n",
              "      <td>S2340-98942015000100005-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>c56.2</td>\n",
              "      <td>carcinoma seroso papilar en ovario izquierdo</td>\n",
              "      <td>94 138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9178</th>\n",
              "      <td>S2340-98942015000100005-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r97.1</td>\n",
              "      <td>elevación de CA 125</td>\n",
              "      <td>413 432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9179</th>\n",
              "      <td>S2340-98942015000100005-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r55</td>\n",
              "      <td>pérdida de conocimiento</td>\n",
              "      <td>959 982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9180</th>\n",
              "      <td>S2340-98942015000100005-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r06.00</td>\n",
              "      <td>disnea</td>\n",
              "      <td>1542 1548</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7209 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81fb1c2b-d018-4658-accd-f14161c75fca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81fb1c2b-d018-4658-accd-f14161c75fca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81fb1c2b-d018-4658-accd-f14161c75fca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ],
      "source": [
        "train_dataset = join(dir_dataset,'train','trainX.tsv')\n",
        "test_dataset = join(dir_dataset,'test','testX.tsv')\n",
        "\n",
        "in_train_df = pd.read_csv(train_dataset, sep = '\\t', header=None)\n",
        "in_test_df = pd.read_csv(test_dataset, sep = '\\t', header=None)\n",
        "\n",
        "in_train_df = in_train_df.loc[in_train_df[1]=='DIAGNOSTICO']\n",
        "in_test_df = in_test_df.loc[in_test_df[1]=='DIAGNOSTICO']\n",
        "\n",
        "in_train_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs = pd.unique(in_train_df[0])\n",
        "train_df = pd.DataFrame(columns=['text','labels'])\n",
        "for i in unique_docs:\n",
        "  doc_df = in_train_df.loc[in_train_df[0]==i]\n",
        "  labels = []\n",
        "  for index_, row_ in doc_df.iterrows():\n",
        "    label = row_[2]\n",
        "    labels.append(label)\n",
        "  labels = max(set(labels), key = labels.count)\n",
        "  train_df = train_df.append({'text':i, 'labels':labels}, ignore_index=True)\n",
        "\n",
        "\n",
        "unique_docs = pd.unique(in_test_df[0])\n",
        "test_df = pd.DataFrame(columns=['text','labels'])\n",
        "for i in unique_docs:\n",
        "  doc_df = in_test_df.loc[in_test_df[0]==i]\n",
        "  labels = []\n",
        "  for index_, row_ in doc_df.iterrows():\n",
        "    label = row_[2]\n",
        "    labels.append(label)\n",
        "  labels = max(set(labels), key = labels.count)\n",
        "  test_df = test_df.append({'text':i, 'labels':labels}, ignore_index=True)\n",
        "test_df\n"
      ],
      "metadata": {
        "id": "qBQV_nx89IhY",
        "outputId": "d9b24308-65b9-4210-bbc5-70764192f054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          text    labels\n",
              "0    S0004-06142005000500011-1  s22.49xa\n",
              "1    S0004-06142005000900014-1     n32.9\n",
              "2    S0004-06142006000100010-1    n28.89\n",
              "3    S0004-06142006000500012-1     c64.9\n",
              "4    S0004-06142006000600014-1     n36.8\n",
              "..                         ...       ...\n",
              "245  S1887-85712016000100005-1     t14.8\n",
              "246  S1887-85712017000100004-1   s72.401\n",
              "247  S1888-75462016000400180-1   s22.41x\n",
              "248  S1888-75462017000100042-1   m25.439\n",
              "249  S2254-28842014000300010-1     m54.5\n",
              "\n",
              "[250 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb0dd8cb-041f-4e28-8b92-c09675c12c10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S0004-06142005000500011-1</td>\n",
              "      <td>s22.49xa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>S0004-06142005000900014-1</td>\n",
              "      <td>n32.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S0004-06142006000100010-1</td>\n",
              "      <td>n28.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>S0004-06142006000500012-1</td>\n",
              "      <td>c64.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>S0004-06142006000600014-1</td>\n",
              "      <td>n36.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>S1887-85712016000100005-1</td>\n",
              "      <td>t14.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>S1887-85712017000100004-1</td>\n",
              "      <td>s72.401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>S1888-75462016000400180-1</td>\n",
              "      <td>s22.41x</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>S1888-75462017000100042-1</td>\n",
              "      <td>m25.439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>S2254-28842014000300010-1</td>\n",
              "      <td>m54.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb0dd8cb-041f-4e28-8b92-c09675c12c10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fb0dd8cb-041f-4e28-8b92-c09675c12c10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fb0dd8cb-041f-4e28-8b92-c09675c12c10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwhvhBd5TSs"
      },
      "source": [
        "Ahora estamos en condiciones de pre-procesar todo lo que tenemos:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_docs = []\n",
        "\n",
        "for doc_name in train_df['text']:\n",
        "  text_file = join(dir_dataset,'train','text_files','%s.txt' % doc_name)\n",
        "  f = open(text_file,'r')\n",
        "  \n",
        "  doc_text=f.read()\n",
        "  plain_text = preprocesar(doc_text)\n",
        "  f.close()\n",
        "\n",
        "  train_docs.append(plain_text)\n",
        "\n",
        "test_docs = []\n",
        "\n",
        "for doc_name in test_df['text']:\n",
        "  text_file = join(dir_dataset,'test','text_files','%s.txt' % doc_name)\n",
        "  f = open(text_file,'r')\n",
        "  \n",
        "  doc_text=f.read()\n",
        "  plain_text = preprocesar(doc_text)\n",
        "  f.close()\n",
        "\n",
        "  test_docs.append(plain_text)"
      ],
      "metadata": {
        "id": "8mHGUxMouZIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fqRBjs05TSs"
      },
      "source": [
        "Hemos pasado de una Series de Pandas, array de NumPy o una **lista de strings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqwc2jIk5TSt"
      },
      "source": [
        "Un poco de nomenclatura: estamos llamando corpus a la colección de textos. Nos referimos también a los textos como documentos. También estamos usando el término lote (*batch*) para referirnos a un (sub)conjunto de documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW0Dv99-5TSt"
      },
      "source": [
        "## Vocabulario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnM1otZ95TSt"
      },
      "source": [
        "Este paso es importante. Aquí definimos y limitamos la tókenes que vamos a utilizar. El lenguaje es infinito, para convertirlo en un problema tratable muchas veces los que hacemos es reducirlo. Clave para varias prácticas de reducción es contar las frecuencias de los tókenes, esto es, cuántas veces aparece cada tóken en todo el corpus. Como mencionamos las palabras más frecuentes no aportan mucha información y las más infrecuentes si bien son las que más información tienen no llegarán a ser representativas para nuestro modelo. Descartar palabras poco frecuentes también afecta a errores ortográficos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9huyb6Q5TS1"
      },
      "source": [
        "### Implementación\n",
        "\n",
        "Veamos cómo acomodamos lo que hemos visto ahora en la clase `Vocab`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "tpnZyyWq5TS1"
      },
      "outputs": [],
      "source": [
        "# versión 4\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "\n",
        "class Vocab():\n",
        "    @property\n",
        "    def índice_relleno(self):\n",
        "        return self.mapeo.get(self.tóken_relleno)\n",
        "    \n",
        "    def __init__(self, tóken_desconocido='<unk>', tóken_relleno='<pad>', frecuencia_mínima=0.0, frecuencia_máxima=0.1,\n",
        "                 longitud_mínima=1, longitud_máxima=np.inf, stop_words=['paciente', 'síntoma', 'tratamiento'], límite_vocabulario=None):\n",
        "        \n",
        "        self.tóken_desconocido = tóken_desconocido\n",
        "        self.tóken_relleno = tóken_relleno\n",
        "        self.frecuencia_mínima = frecuencia_mínima\n",
        "        self.frecuencia_máxima = frecuencia_máxima\n",
        "        self.longitud_mínima = longitud_mínima\n",
        "        self.longitud_máxima = longitud_máxima\n",
        "        self.stop_words = stop_words\n",
        "        self.límite_vocabulario = límite_vocabulario\n",
        "    \n",
        "    # ningún cambio aquí\n",
        "    def reducir_vocabulario(self, lote):\n",
        "        contador_absoluto = Counter(chain(*lote))\n",
        "        \n",
        "        contador_documentos = Counter()\n",
        "        \n",
        "        for doc in lote:\n",
        "            contador_documentos.update(set(doc))\n",
        "        \n",
        "        # frecuencia mínima\n",
        "        if isinstance(self.frecuencia_mínima, int): # frecuencia de tóken\n",
        "            vocabulario_mín = [tóken for tóken, frecuencia in contador_absoluto.most_common() if frecuencia >= self.frecuencia_mínima]\n",
        "        else: # frecuencia de documento\n",
        "            vocabulario_mín = [tóken for tóken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) >= self.frecuencia_mínima]\n",
        "        \n",
        "        # frecuencia máxima\n",
        "        if isinstance(self.frecuencia_máxima, int): # frecuencia de tóken\n",
        "            vocabulario_máx = [tóken for tóken, frecuencia in contador_absoluto.most_common() if self.frecuencia_máxima >= frecuencia]\n",
        "        else: # frecuencia de documento\n",
        "            vocabulario_máx = [tóken for tóken, frecuencia in contador_documentos.most_common() if self.frecuencia_máxima >= frecuencia/len(lote)]\n",
        "\n",
        "        # intersección de vocabulario_mín y vocabulario_máx preservando el órden\n",
        "        vocabulario = [tóken for tóken in vocabulario_mín if tóken in vocabulario_máx]\n",
        "\n",
        "        # longitud\n",
        "        vocabulario = [tóken for tóken in vocabulario if self.longitud_máxima >= len(tóken) >= self.longitud_mínima]\n",
        "        \n",
        "        # stop words\n",
        "        vocabulario = [tóken for tóken in vocabulario if tóken not in self.stop_words]\n",
        "        \n",
        "        # límite\n",
        "        vocabulario = vocabulario[:self.límite_vocabulario]\n",
        "        \n",
        "        return vocabulario\n",
        "        \n",
        "    def fit(self, lote):\n",
        "        vocabulario = self.reducir_vocabulario(lote)\n",
        "        \n",
        "        if self.tóken_desconocido:\n",
        "            vocabulario.append(self.tóken_desconocido)\n",
        "        \n",
        "        if self.tóken_relleno:\n",
        "            vocabulario.insert(0, self.tóken_relleno)\n",
        "        \n",
        "        self.mapeo = {tóken: índice for índice, tóken in enumerate(vocabulario)}\n",
        "\n",
        "        return self\n",
        "    \n",
        "    # ningún cambio aquí\n",
        "    def transform(self, lote):\n",
        "        if self.tóken_desconocido: # reemplazar\n",
        "            return [[tóken if tóken in self.mapeo else self.tóken_desconocido for tóken in doc] for doc in lote]\n",
        "        else: # ignorar\n",
        "            return [[tóken for tóken in doc if tóken in self.mapeo] for doc in lote]\n",
        "    \n",
        "    # ningún cambio aquí\n",
        "    def tókenes_a_índices(self, lote):\n",
        "        lote = self.transform(lote)\n",
        "        \n",
        "        return [[self.mapeo[tóken] for tóken in doc] for doc in lote]\n",
        "    \n",
        "    # ningún cambio aquí\n",
        "    def índices_a_tókenes(self, lote):\n",
        "        mapeo_inverso = list(self.mapeo.keys())\n",
        "        \n",
        "        return [[mapeo_inverso[índice] for índice in doc] for doc in lote]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.mapeo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaH3jrO35TS2"
      },
      "source": [
        "## El pre-procesamiento hasta ahora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "JmSqBKdi5TS2"
      },
      "outputs": [],
      "source": [
        "v = Vocab().fit(train_docs)\n",
        "\n",
        "train_índices = v.tókenes_a_índices(train_docs)\n",
        "test_índices = v.tókenes_a_índices(test_docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(v)"
      ],
      "metadata": {
        "id": "a84Y95e-AK4w",
        "outputId": "6f669775-7c02-4a0f-95ff-bcdf89a34da3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15531"
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLfbs4m5TS2"
      },
      "source": [
        "Con esto concluye la primera parte. Hay varias librerías que tienen clases que se encargan de efectuar los pasos que hemos visto. Tienen un comportamiento por defecto, que es configurable (los parámetros que hemos visto) y a su vez, personalizable, para reemplazar algunos o todos los pasos por código propio. En general son librerías desarrolladas por angloparlantes, funcionan *out-of-the-box* bien para el inglés; cuando queremos procesar texto en español vale la pena tener más control sobre estos procesos.\n",
        "\n",
        "* [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de scikit-learn.\n",
        "* [TextDataBunch](https://docs.fast.ai/text.data.html#TextDataBunch.from_df) de fast.ai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rf94M9i5TS2"
      },
      "source": [
        "## Pre-procesando las etiquetas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihZc6Mbe5TS2"
      },
      "source": [
        "Las etiquetas del dataset también necesitan ser convertidas a números enteros consecutivos. No lo pensamos para este fin pero `Vocab` sería útil en este aspecto. El único tema es que `Vocab.fit` y demás métodos esperan listas de listas de tókenes y a las etiquetas las encontramos en forma de listas de tókenes simplemente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edWw7hm85TS3"
      },
      "source": [
        "Podemos llevar la columna de las etiquetas a una lista de listas con `train_df['Intencion'].values.reshape(-1,1)`, de manera de poder interfacearlo con `Vocab`. Algo como `train_df[['Intencion']].values` para que Pandas devuelva un `DataFrame` en vez de una `Series` también funcionaría."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "Sx2FPB6T5TS3"
      },
      "outputs": [],
      "source": [
        "train_etiquetas = train_df[['labels']].values\n",
        "test_etiquetas = test_df[['labels']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir94eLey5TS3"
      },
      "source": [
        "Todo lo que tenga que ver con limitación del vocabulario o agregado de tókenes especiales no nos interesa para este caso de uso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "f-F6b9d95TS3"
      },
      "outputs": [],
      "source": [
        "vocabulario_etiquetas = Vocab(tóken_desconocido=None).fit(train_etiquetas)\n",
        "\n",
        "train_etiquetas = vocabulario_etiquetas.tókenes_a_índices(train_etiquetas)\n",
        "test_etiquetas = vocabulario_etiquetas.tókenes_a_índices(test_etiquetas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBMGgESH5TS3"
      },
      "source": [
        "Ya casi estamos. Solo debemos reconvertir a las etiquetas en una lista de índices (su dimensión original) con un recurso que ya conocemos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "wf6hIydG5TS3",
        "outputId": "22218623-03b7-4f24-86c9-4abc92cb9b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[45, 46, 1, 47, 2, 3, 48, 4, 49, 50]"
            ]
          },
          "metadata": {},
          "execution_count": 267
        }
      ],
      "source": [
        "train_etiquetas_índices = list(chain(*train_etiquetas))\n",
        "test_etiquetas_índices = list(chain(*test_etiquetas))\n",
        "\n",
        "train_etiquetas_índices[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasta ahora llegamos a convertir esto\n",
        "\n",
        "```python\n",
        "[\n",
        "    'que se requiere para un prestamo personal',\n",
        "    'me piden mi numero de cuenta es mi cbu',\n",
        "]\n",
        "```\n",
        " \n",
        "en esto\n",
        "\n",
        "```python\n",
        "[\n",
        "    [4160, 4683, 4484, 3703, 5294, 4011, 3825],\n",
        "    [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950],\n",
        "]\n",
        "```\n",
        "\n",
        "donde dijimos que las partes fundamentales son la tokenización —separar a los documentos en unidades de información— y la numericalización —el asignarle a cada uno de los tókenes un número, más que nada para que la computadora, que gusta mucho de los números, sea feliz—.\n",
        "\n",
        "También habíamos dicho que **un tóken es un atributo** pero no dijimos mucho más al respecto. Veamos cómo puede ser esto. La tarea de ejemplo es clasificar documentos. Estamos acustumbrados a tener muestras y etiquetas como `X` e `y` en las que la primera es una matriz de muestras (filas) y atributos (columnas), y la segunda suele ser una columna. Cuando el dataset está sin pre-procesar tenemos las muestras (filas) pero no los atributos (columnas), por lo general tenemos una única columna con los documentos en forma de strings, lo que mucha forma de atributos no tiene. \n",
        "\n",
        "Ahora que hemos pre-procesado el texto estamos a un paso de obtener los atributos. La función de los atributos es describir o caracterizar a las muestras. El modelo lee estos atributos para realizar inferencias. Hay distintas maneras de describir a los documentos, algunas más sofisticadas que otras, una intuitiva es aprovechar que los tókenes están numerados desde 0 hasta L (`len(vocabulario)`) y otorgarle una columna a cada uno en la matriz de atributos de tamaño N x L (donde N es la cantidad de muestras).\n",
        "\n",
        "Hecho esto, solo resta contar cuántas veces aparece cada tóken en cada documento y asentarlo en la matriz.\n",
        "\n",
        "```\n",
        "                  |  bien  hola  si    todo\n",
        "-------------------------------------------\n",
        "'hola todo bien'  |  1     1     0     1\n",
        "'si bien bien'    |  2     0     1     0\n",
        "```\n",
        "\n",
        "Como comentario, esta forma de describir los documentos ignora enteramente el órden de los tókenes, sabemos que el sentido de una oración puede cambir completamente si cambiamos algunas palabras de lugar. Para el problema en cuestión, no parece ser tan grave ya que para clasificar una pregunta podría bastar con reconocer algunas palabras claves como *cambio* y *clave* o *requisito* y *préstamo*.\n",
        "\n",
        "Ver [tf-idf](https://es.wikipedia.org/wiki/Tf-idf)."
      ],
      "metadata": {
        "id": "1QPA5ME8BBvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BONUS (Si hay tiempo)\n",
        "## PyTorch\n",
        "\n",
        "El típico bucle de entrenamiento de PyTorch tiene esta pinta.\n",
        "\n",
        "```python\n",
        "for época in range(N_ÉPOCAS):\n",
        "    for lote in datos_entrenamiento:\n",
        "        # reseteamos los gradientes\n",
        "        optimizador.zero_grad()\n",
        "        \n",
        "        predicciones = red_neuronal(lote.X)\n",
        "        pérdida = criterio(predicciones, lote.y)\n",
        "        \n",
        "        # calculamos los gradientes\n",
        "        pérdida.backward()\n",
        "    \n",
        "        # aplicamos los gradientes\n",
        "        optimizador.step()\n",
        "```\n",
        "\n",
        "Recordemos que a diferencia de otros modelos las redes neuronales revisitan varias veces el dataset, en lo que se llaman épocas, cada época es un recorrido por todas las muestras de entrenamiento.\n",
        "\n",
        "En una época el dataset se puede mostrar entero, de a una muestra, o como es común hoy en día de a grupos o lotes (*batches*). La experiencia mostró que es útil variar el orden de las muestras en cada época."
      ],
      "metadata": {
        "id": "YFH2OiagCeMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch provee ciertas facilidades para el manejo de los datos con las clases definidas en [torch.utils.data](https://pytorch.org/docs/stable/data.html) a ser:\n",
        "1. `Dataset`. Organiza los datos. Le pasamos un número o índice de muestra y nos devuelve la muestra usualmente como una tupla `(atributos, etiqueta)`.\n",
        "1. `Sampler`. Salvo que lo queramos de otra manera, se encarga de brindar un orden aleatoreo de los índices del dataset; uno diferente cada vez que le preguntamos.\n",
        "1. `BatchSampler`. Por defecto, se inicializa con un `Sampler` y el tamaño de lote. Se encarga de armar grupos de índices; diferentes cada vez que le preguntamos.\n",
        "1. `DataLoader`. Valiéndose de los grupos de índices de `BatchSampler`, obtiene muestras de `Dataset`. De esta manera para cada época devuelve lotes de muestras al azar.  \n",
        "\n",
        "Por `Sampler` y `BatchSampler` no nos detendremos ya el comportamiento por defecto, que es barajar el dataset en cada época y armar lotes del mismo tamaño es todo lo que necesitamos."
      ],
      "metadata": {
        "id": "ohYY_AXrCiWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader\n",
        "\n",
        "`DataLoader` es un *iterable*. Los iterables son colecciones de elementos que se pueden recorrer; implementan el método `__iter__`, del que se espera que devuelva un objeto *iterador* (`iterador = iter(iterable)`). A su vez el iterador implementa el método `__next__` que se encarga devolver secuencialmente los elementos de la colección hasta que se agota; una vez que esto sucede el iterador debe ser descartado y en todo caso le pedimos al iterable que nos arme un nuevo iterador. Cuando usamos la construcción `for ítem in iterable`, el intérprete de Python implícitamente obtiene un iterador.\n",
        "\n",
        "Ver la [sección de interables](https://docs.python.org/3/tutorial/classes.html#iterators) en el tutorial de Python."
      ],
      "metadata": {
        "id": "t_FSASS-DXDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista = iter(['uno','dos'])\n",
        "\n",
        "next(lista)"
      ],
      "metadata": {
        "id": "Z_OVpJQZDP51",
        "outputId": "d7d69a33-2894-4152-ea9c-ddabdcd7a2dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'uno'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=20, shuffle=True)"
      ],
      "metadata": {
        "id": "G_OAWDWMDb4I"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Le* estamos diciendo a `DataLoader` que queremos lotes de 32 muestras (`batch_size`) y que el armado de los lotes sea aleatorio (`shuffle`). "
      ],
      "metadata": {
        "id": "DuKXZv-SDiuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rellenar_documentos(lote, largos, índice_relleno):\n",
        "    máximo_largo = max(largos)\n",
        "    \n",
        "    return [doc + [índice_relleno] * (máximo_largo - largos[i]) for i, doc in enumerate(lote)]"
      ],
      "metadata": {
        "id": "tt_QMHSzErCz"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AtriDicc():\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.__dict__ = dict(*args, **kwargs)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return repr(self.__dict__)"
      ],
      "metadata": {
        "id": "A8F-llKhE1Xh"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Textset(Dataset):\n",
        "    def __init__(self, documentos, etiquetas=None):\n",
        "            \n",
        "        self.documentos = documentos\n",
        "        self.etiquetas  = etiquetas or np.full(len(documentos), np.nan)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documentos)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        return AtriDicc(\n",
        "            documento = self.documentos[item],\n",
        "            largo = len(self.documentos[item]),\n",
        "            etiqueta =  self.etiquetas[item],\n",
        "        )"
      ],
      "metadata": {
        "id": "kt5xMZnMGqfl"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Textset(train_índices, train_etiquetas_índices)\n",
        "len(train_ds)\n",
        "train_ds[400].documento[:10]"
      ],
      "metadata": {
        "id": "fL85hfOcG4du",
        "outputId": "f1d9ad0c-e4c7-4169-fc97-f993890da611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 14054, 160, 1, 28, 35, 2282, 7438, 112, 43]"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función *collate*\n",
        "\n",
        "*Collate* significa juntar diferentes piezas de información para ver sus similaridades y diferencias, también puede ser colectar y organizar las hojas de un reporte, un libro. En el contexto de `DataLoader` quiere decir arreglar el lote. Entonces esta función recibe una lista de elementos del `Dataset`, en nuestro caso una lista de de `AtriDicc`s, y debe devolver el lote en una forma útil y en lo posible realizar conversiones a tensores.\n",
        "\n",
        "`DataLoader` posee una *collate function* por defecto que utiliza internamente y que en muchos casos funciona correctamente, pero otros como ahora que tenemos documentos de distinto largo nos toca definir una función propia."
      ],
      "metadata": {
        "id": "0ZXZkZxVHMy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rellenar_lote(lote):\n",
        "    \"\"\"Prepara lotes para ingresar a nn.Embedding\"\"\"\n",
        "    documentos = [elemento.documento for elemento in lote]\n",
        "    largos     = [elemento.largo     for elemento in lote]\n",
        "    etiquetas  = [elemento.etiqueta  for elemento in lote]\n",
        "\n",
        "    rellenos = rellenar_documentos(documentos, largos, v.índice_relleno)\n",
        "    \n",
        "    return AtriDicc(\n",
        "        documentos = torch.tensor(rellenos),\n",
        "        etiquetas  = torch.tensor(etiquetas),\n",
        "    )"
      ],
      "metadata": {
        "id": "Lvff9ZMNHOBI"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando instanciamos un `DataLoader` le pasamos la función que acabamos de definir."
      ],
      "metadata": {
        "id": "3_0EuJTcHTAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_ds, collate_fn=rellenar_lote, batch_size=3, shuffle=True)"
      ],
      "metadata": {
        "id": "YnRLplLEHTyq"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "un_lote = next(iter(train_dl))\n",
        "un_lote.documentos"
      ],
      "metadata": {
        "id": "APqJ4M8THYZt",
        "outputId": "2076e473-f695-491e-a19e-918a731ad981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  53,  834,    1,  ...,   32, 5042,  291],\n",
              "        [   2,   12,  357,  ...,    0,    0,    0],\n",
              "        [ 402,  120,   53,  ...,    0,    0,    0]])"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "un_lote.etiquetas"
      ],
      "metadata": {
        "id": "_jblOdHLHcBX",
        "outputId": "32c1c564-7dc4-442f-c9e6-4349e3d5fd1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([185,   1,  62])"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Uqgp1L5TS3"
      },
      "source": [
        "## Fuentes consultadas\n",
        "\n",
        "https://github.com/matiasbattocchia/datitos/blob/master/_notebooks/2020-07-23-Preprocesamiento-de-texto-para-NLP-parte-1.ipynb\n",
        "https://github.com/matiasbattocchia/datitos/blob/master/_notebooks/2020-07-30-Preprocesamiento-de-texto-para-NLP-parte-2.ipynb"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "name": "pytorch-gpu.1-4.m50",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_3/Clases/Semana_26_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH0raAsM5TSa"
      },
      "source": [
        "Este cuaderno es una adaptación del trabajo previo de:\n",
        "https://github.com/matiasbattocchia/datitos\n",
        "\n",
        "\n",
        "Vamos a hacer un recorrido por los pasos básicos del pre-procesamiento de texto. Estos pasos son necesarios para transformar texto del lenguaje humano a un formato legible para máquinas para su posterior procesamiento.\n",
        "\n",
        "Veremos cómo realizar estos pasos con código propio, para mayor entendimiento de lo que está sucediendo, y con [spaCy](https://spacy.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsgRit3B5TSb"
      },
      "source": [
        "En concreto, los pasos son:\n",
        "\n",
        "1. **Limpieza**, la remoción del contenido no deseado.\n",
        "2. **Normalización**, la conversión diferentes formas a una sola. \n",
        "3. **Tokenización**, la separación del texto en tókenes (unidades mínimas, por ejemplo palabras).\n",
        "4. Separación en **conjuntos de datos**: entrenamiento, validación, prueba.\n",
        "5. Generación del **vocabulario**, la lista de tókenes conocidos.\n",
        "6. **Numericalización**, el mapeo de tókenes a números enteros.\n",
        "\n",
        "Nota: El órden de los primeros tres pasos (limpieza, normalización, tokenización) puede variar según conveniencia. El resto de los pasos mantiene el órden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaVz_Tun5TSc"
      },
      "source": [
        "## Dataset de ejemplo\n",
        "\n",
        "¿Qué sería de esta publicación sin algunos ejemplos? En nuestro caso vamos a utilizar el dataset CodiEsp. Se trata de un corpus de textos clínicos en español:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/3837305/files/codiesp.zip\n",
        "!unzip codiesp.zip >> /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nb7tDJoApCV",
        "outputId": "d62960b1-41a0-4768-817e-cc2f83fc6b22"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-11 10:26:26--  https://zenodo.org/record/3837305/files/codiesp.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11239591 (11M) [application/octet-stream]\n",
            "Saving to: ‘codiesp.zip.2’\n",
            "\n",
            "codiesp.zip.2       100%[===================>]  10.72M  5.78MB/s    in 1.9s    \n",
            "\n",
            "2023-02-11 10:26:29 (5.78 MB/s) - ‘codiesp.zip.2’ saved [11239591/11239591]\n",
            "\n",
            "replace final_dataset_v4_to_publish/README.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w8CNvfcd5TSd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from os.path import join\n",
        "\n",
        "dir_dataset = 'final_dataset_v4_to_publish'\n",
        "train_dataset = join(dir_dataset,'train','trainX.tsv')\n",
        "\n",
        "df = pd.read_csv(train_dataset, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A7pFpRdS5TSe",
        "outputId": "22213c26-f256-451b-a6d8-19fcbc813033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      S0004-06142005000700014-1  PROCEDIMIENTO  bw03zzz  \\\n",
              "8097  S1139-76322010000600010-1  DIAGNOSTICO    h01.8     \n",
              "830   S0004-06142009000800010-1  DIAGNOSTICO    c79.52    \n",
              "1738  S0210-48062009000900019-1  PROCEDIMIENTO  b410      \n",
              "1866  S0210-56912007000900007-1  DIAGNOSTICO    i82.40    \n",
              "1302  S0210-48062006000300014-1  PROCEDIMIENTO  0t90      \n",
              "5944  S1130-01082010000700014-1  DIAGNOSTICO    r19.7     \n",
              "8728  S1699-695X2014000200012-1  PROCEDIMIENTO  0cb83zx   \n",
              "1163  S0210-48062005000700012-1  DIAGNOSTICO    a23.9     \n",
              "3254  S0212-71992005001000009-1  DIAGNOSTICO    r50.9     \n",
              "3215  S0212-71992005000600008-1  DIAGNOSTICO    m54.9     \n",
              "\n",
              "                                         Rx tórax            2163 2171  \n",
              "8097  párpado ulceraciones                         387 394;419 431      \n",
              "830   infiltración de la médula ósea por el tumor  2493 2536            \n",
              "1738  arteriografía aorta abdominal                1194 1207;1257 1272  \n",
              "1866  trombosis venosa profunda                    319 344              \n",
              "1302  ambas fosas renales nefrostomía percutánea   1718 1737;1938 1960  \n",
              "5944  diarrea                                      184 191              \n",
              "8728  PAAF glándula parótida derecha               640 644;707 732      \n",
              "1163  Brucella                                     491 499              \n",
              "3254  fiebre                                       1735 1741            \n",
              "3215  Dolor columna vertebral                      734 739;761 778      "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dec13e7f-c83d-48a8-ab74-07a92ddc2198\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S0004-06142005000700014-1</th>\n",
              "      <th>PROCEDIMIENTO</th>\n",
              "      <th>bw03zzz</th>\n",
              "      <th>Rx tórax</th>\n",
              "      <th>2163 2171</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8097</th>\n",
              "      <td>S1139-76322010000600010-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>h01.8</td>\n",
              "      <td>párpado ulceraciones</td>\n",
              "      <td>387 394;419 431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>830</th>\n",
              "      <td>S0004-06142009000800010-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>c79.52</td>\n",
              "      <td>infiltración de la médula ósea por el tumor</td>\n",
              "      <td>2493 2536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1738</th>\n",
              "      <td>S0210-48062009000900019-1</td>\n",
              "      <td>PROCEDIMIENTO</td>\n",
              "      <td>b410</td>\n",
              "      <td>arteriografía aorta abdominal</td>\n",
              "      <td>1194 1207;1257 1272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1866</th>\n",
              "      <td>S0210-56912007000900007-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>i82.40</td>\n",
              "      <td>trombosis venosa profunda</td>\n",
              "      <td>319 344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302</th>\n",
              "      <td>S0210-48062006000300014-1</td>\n",
              "      <td>PROCEDIMIENTO</td>\n",
              "      <td>0t90</td>\n",
              "      <td>ambas fosas renales nefrostomía percutánea</td>\n",
              "      <td>1718 1737;1938 1960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5944</th>\n",
              "      <td>S1130-01082010000700014-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r19.7</td>\n",
              "      <td>diarrea</td>\n",
              "      <td>184 191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8728</th>\n",
              "      <td>S1699-695X2014000200012-1</td>\n",
              "      <td>PROCEDIMIENTO</td>\n",
              "      <td>0cb83zx</td>\n",
              "      <td>PAAF glándula parótida derecha</td>\n",
              "      <td>640 644;707 732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1163</th>\n",
              "      <td>S0210-48062005000700012-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>a23.9</td>\n",
              "      <td>Brucella</td>\n",
              "      <td>491 499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3254</th>\n",
              "      <td>S0212-71992005001000009-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>r50.9</td>\n",
              "      <td>fiebre</td>\n",
              "      <td>1735 1741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3215</th>\n",
              "      <td>S0212-71992005000600008-1</td>\n",
              "      <td>DIAGNOSTICO</td>\n",
              "      <td>m54.9</td>\n",
              "      <td>Dolor columna vertebral</td>\n",
              "      <td>734 739;761 778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dec13e7f-c83d-48a8-ab74-07a92ddc2198')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dec13e7f-c83d-48a8-ab74-07a92ddc2198 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dec13e7f-c83d-48a8-ab74-07a92ddc2198');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "with pd.option_context('display.max_colwidth', -1):\n",
        "    \n",
        "    display(df.sample(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text_file = join(dir_dataset,'train','text_files','S0212-71992005000600008-1.txt')\n",
        "f = open(example_text_file,'r')\n",
        "example_text=f.read()\n",
        "print(example_text)\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "6KrKOUM2Bd31",
        "outputId": "a7a1f6b9-4135-48a4-c822-57cfd9b92562",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Varón de 71 años, que ingresó en el servicio de Medicina Interna para estudio de síndrome constitucional. En el curso de los 6 meses anteriores había experimentado deterioro progresivo manifestado por astenia, anorexia, pérdida de unos 20 kg de peso e incapacidad para la deambulación. Entre los antecedentes personales destacaba haber sido fumador y bebedor importante hasta 23 años antes y presentar diabetes mellitus tipo 2 y prostatismo de aproximadamente 5 años de evolución. A la exploración física se encontró un paciente que impresionaba de enfermedad crónica y bradipsíquico, con palidez de piel y mucosas y disminución generalizada de fuerza en las 4 extremidades. No adenopatías. Crepitantes en la base pulmonar izquierda. Dolor a la percusión de la columna vertebral, más intenso a los niveles cervical y dorsal. Abdomen blando y depresible, doloroso a la palpación en hipocondrio derecho y zona periumbilical, donde se aprecia a la inspección y se palpa un nódulo de consistencia dura, fijo a planos profundos y de aproximadamente 3 x 3 cm, cubierto de piel de características normales. No visceromegalias. Tacto rectal con una próstata discretamente aumentada de tamaño, sin irregularidades, siendo el resto de la exploración física normal.\n",
            "De la analítica destaca: ALT: 47 U/l; GGT: 259 U/l; Fosfatasas alcalinas: 277 U/l; Uratos 2,29 mg/dl. Moderada leucocitosis y trombocitosis. VSG 41 mm a la 1ª hora. Resto del protocolo general que incluye TSH dentro de la normalidad. Ferritina: 515 ng/ml; ausencia de componente M en el proteinograma; alfafetoproteína normal; PSA: 11,5 ng/ml (valores normales de 0,4 a 4); Antígeno carcinoembrionario: 221 ng/ml (valores normales hasta 5); y Ca 19,9 de 188 U/ml (valores normales hasta 37). No se realizó uricosuria de 24 horas.\n",
            "La PAAF del nódulo periumbilical demostró la presencia de células malignas sugestivas de adenocarcinoma y tras ello se procedió a la búsqueda del tumor primario con: Radiografía de tórax que mostró un infiltrado alveolar en el lóbulo inferior izquierdo; ecografía abdominal: poco valorable por distensión aérea abdominal, si bien se aprecia el nódulo periumbilical. La gammagrafía ósea objetivó múltiples acúmulos hipercaptantes en esqueleto axial y periférico. La TAC tóraco-abdominal reveló la existencia de un nódulo de 1,4 x 1,6 cm en lóbulo superior derecho, que contacta con la pleura visceral, que parecía engrosada y con lesiones líticas en el arco costal posterior. Metástasis líticas a nivel de columna dorsal y esternón. Mínimo derrame pleural bilateral. Múltiples lesiones focales hepáticas en segmento III con patrón vascular compatibles con hemangiomas. Lesión de 1,1 cm en segmento IV probablemente metastásica. Nódulo de 1,4 cm en glándula suprarrenal izquierda sugestiva de adenoma. Lesión hipodensa en cola del páncreas de 1,7 x 1 cm posiblemente en relación con tumor primario. Implantes tumorales en partes blandas, una de ellas a nivel periumbilical, compatible con nódulo de la hermana María José, y otra en músculo iliaco derecho, con infiltración ósea metastásica en pelvis, sacro y vértebras lumbares. Tras la realización de las técnicas de imagen referidas, que no definen un claro origen de su carcinomatosis, a las 2 semanas de estancia hospitalaria se traslada a la unidad de Medicina Paliativa, donde fallece.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-yzBMbj5TSf"
      },
      "source": [
        "## Expresiones regulares\n",
        "\n",
        "Si las expresiones regulares no te resultan familiares entonces vale la pena estudiarlas brevemente, ya que las usaremos. Podés mirar este [tutorial](https://robologs.net/2019/05/05/como-utilizar-expresiones-regulares-regex-en-python) que encontramos en la web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bz7O8kr95TSf"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxN89Odx5TSg"
      },
      "source": [
        "## Limpieza\n",
        "\n",
        "Muchas técnicas modernas no realizan limpieza alguna. Dependiendo de lo que queramos hacer tal vez convenga deshacernos de algunos elementos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uG0f8Tcn5TSg"
      },
      "outputs": [],
      "source": [
        "def limpiar(texto):\n",
        "    puntuación = r'[,;.:¡!¿?@#$%&[\\](){}<>~=+\\-*/|\\\\_^`\"\\']'\n",
        "    \n",
        "    # signos de puntuación\n",
        "    texto = re.sub(puntuación, ' ', texto)\n",
        "\n",
        "    return texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKHcP8GJ5TSh"
      },
      "source": [
        "En esta función substituimos los signos de puntuación\n",
        "    \n",
        "    , ; . : ¡ ! ¿ ? @ # $ % & [ ] ( ) { } < > ~ = + - * / | \\ _ ^ ` \" '\n",
        "\n",
        "por espacios (me gusta más; usar string vacío `''` para eliminarlos) medieante expresiones regulares (algunos caracteres tuvieron que ser escapados anteponiendo `\\` por tener un significado especial para la expresión regular). Hacemos lo mismo con los dígitos. Veamos un ejemplo de funcionamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "e0CIM4rO5TSi",
        "outputId": "bd9e1eb3-6c2d-4f84-8522-5dacbbf70486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hoy 13 trabajan '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "limpiar('hoy 13 trabajan?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEip6nU65TSi"
      },
      "source": [
        "Otros elementos que podríamos pensar en remover son caracteres invisibles, espacios redundantes. Veremos que esto en particular también puede ser resulto en la tokenización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7dLdL05TSj"
      },
      "source": [
        "## Normalización\n",
        "\n",
        "Normalizar es la tarea de llevar lo que puede ser expresado de múltiples maneras como fechas, números y abreviaturas a una única forma. Por ejemplo\n",
        "\n",
        "     13/03/30 -> trece de marzo de dos mil treinta\n",
        "     DC -> departamento de computación\n",
        "\n",
        "Se trata de una práctica clásica de la época de los modelos de lenguaje probabilísticos, que intentaban reducir lo más posible la cantidad de palabras. En cierta forma 1 palabra = 1 atributo (lo que en los '90s conocimos como convertibilidad). Elegir atributos es ingeniería de atributos, la parte central del *machine learning*, y lo justamente lo que el *deep learning* busca automatizar.\n",
        "\n",
        "Sin embargo hay una normalización muy común hoy, el **convertir todo el texto a minúsculas**. En el caso del español, una normalización común es la **remoción de tildes**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5JT_E6ap5TSj"
      },
      "outputs": [],
      "source": [
        "def normalizar(texto):\n",
        "    # todo a minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # tildes y diacríticas\n",
        "    texto = re.sub('á', 'a', texto)\n",
        "    texto = re.sub('é', 'e', texto)\n",
        "    texto = re.sub('í', 'i', texto)\n",
        "    texto = re.sub('ó', 'o', texto)\n",
        "    texto = re.sub('ú', 'u', texto)\n",
        "    texto = re.sub('ü', 'u', texto)\n",
        "    texto = re.sub('ñ', 'n', texto)\n",
        "\n",
        "    return texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SkFF96gW5TSk",
        "outputId": "03606290-0fb8-4994-d4f5-d32e5baed1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me podran dar informacion de un prestamo personal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "normalizar('Me podrán dar información de un préstamo personal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYm73iir5TSl"
      },
      "source": [
        "Hay una librería llamada [unidecode](https://pypi.org/project/Unidecode) que realiza transliteración: representa letras o palabras de un alfabeto en otro, útil si tenemos caracteres en ruso (cirílico) o chino (caracteres Han), aún útil para el alfabeto latino cuando queremos pasar de Unicode a ASCII (lo que substituiría las tildes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6rSyEYR05TSl",
        "outputId": "1446b0b1-f80e-4fef-baa0-53e1ea0a7b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Me podran dar informacion de un prestamo personal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "\n",
        "unidecode('Me podrán dar información de un préstamo personal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjZ365Jv5TSm"
      },
      "source": [
        "Una normalización que vale la pena intentar con este dataset es la **correción ortográfica** con un paquete como [pyspellchecker](https://pypi.org/project/pyspellchecker). Quizás con artículos de diarios en los que la redacción está más cuidada esto no valga la pena, pero en contextos más informales como este, conversaciones por char, Twitter, las palabras mal escritas en realidad refieren a una sola palabra y no a distintos significados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBjydBHV5TSm"
      },
      "source": [
        "## Tokenización\n",
        "\n",
        "Tokenizar es separar el texto en partes más pequeñas llamadas tókenes. Una unidad muy común es la palabras pero depende de lo que queramos hacer, si es que no hemos eliminado a los signos de puntuación estos también serían tókenes. Las palabras frecuentemente están compuestas por una raíz, prefijo y/o sufijo, por lo que podríamos decidir separarlos también. En inglés es común separar `it's` en `it` y `'s`, si bien en español esta situación no es común.\n",
        "\n",
        "A diferencia de la limpieza y la normalización, la tokenización **es un paso indispesable** en la preparación de texto para su procesamiento.\n",
        "\n",
        "Para el dataset en cuestión la tokenización es simple, vamos a separar seǵun espacios y demás caracteres invisibles como `\\t` (tabulación) y `\\n` (salto de línea). De haber signos de puntuación, pro ejemplo si quisiéramos procesar un documento extenso en oraciones, el proceso es más complejo ya que `final.` tiene un punto en vez de un espacio, y no siempre los puntos demarcan el final de un tóken como en `A.M.` y `P.M.`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR7vfvrv5TSn"
      },
      "source": [
        "Debemos definir si elementos como los signos de puntuación son tókenes o si simplemente delimitan palabras o tókenes, en cuyo caso desaparecerían en el proceso. Mismo con los caracteres invisibles, si estuviésemos haciendo un modelo que programe en Python, la indentación es fundamental y deberiera mantenerse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGiDceSb5TSn"
      },
      "outputs": [],
      "source": [
        "def tokenizar(texto):\n",
        "    # IMPORTANTE: podría devolver una lista vacía\n",
        "    return [tóken for tóken in texto.split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjzVVT5Y5TSo"
      },
      "source": [
        "`split` también se encarga de los caracteres invisibles repetidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNgFCDas5TSo",
        "outputId": "af062f1a-709b-44e8-ad27-7cfc5ca9b525"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hola', 'vengo', 'a', 'flotar']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizar('hola vengo       a flotar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd6jzw-D5TSo"
      },
      "source": [
        "Acá estamos cambiando el tipo de datos, ya que de un string hemos pasado a una lista de strings.\n",
        "\n",
        "Si la expresión dentro de la función no te resulta familiar, es una construcción llamada *list comprehension* y es una manera muy efectiva de armar una lista. Es lo mismo que hacer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEAwLTw_5TSp",
        "outputId": "836b1275-bb1c-4198-eead-d636c2c29093"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lista = []\n",
        "\n",
        "for i in range(10):\n",
        "    lista.append(i)\n",
        "    \n",
        "lista"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICI5025z5TSp"
      },
      "source": [
        "pero de una manera más expresiva y también más eficiente (está optimizado por el lenguaje)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcE1dH0S5TSq",
        "outputId": "db68fa8a-24b7-4f98-dbdb-82b6d6097396"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[i for i in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6lCqs-b5TSq"
      },
      "source": [
        "Varios modelos de lenguaje utilizan caracteres en vez de palabras como tókenes, esto es útil por varios motivos que listaremos más adelante. Otros utilizan partes de palabras como sílabas (las partes se determinan estadísticamente). Ver https://arxiv.org/pdf/1508.07909.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj4SLAtg5TSr"
      },
      "source": [
        "### Tokenización utilizando alguna librería\n",
        "\n",
        "    pip install spacy\n",
        "    python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQunLCt35TSr",
        "outputId": "e9958692-2ce3-4fec-d1f5-ad3916ba3c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Esto', 'es', 'una', 'frase', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "doc = nlp('Esto es una frase.')\n",
        "\n",
        "print([tóken.text for tóken in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdJhCCNs5TSr"
      },
      "source": [
        "## Otros pre-procesos\n",
        "\n",
        "Clásicamente se aplicaban alguno de estos para reducir aún más la cantidad de palabras:\n",
        "\n",
        "#### *Stemming*\n",
        "\n",
        "*Stem*, de raíz, reduce la inflección de las palabras, mapeando un grupo de palabras a la misma raíz, sin importar si la raíz es una palabras válida en el lenguaje.\n",
        "\n",
        "     caminando, caminar, camino -> camin\n",
        "\n",
        "#### *Lemmatization*\n",
        "\n",
        "A diferencia del *stemming*, la lematización reduce las palabras inflexadas a palabras que pertenecen al lenguaje. La raíz pasa a llamarse *lema*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSfSPyC_5TSr"
      },
      "source": [
        "## Primera parte del pre-procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHPYRiat5TSr"
      },
      "outputs": [],
      "source": [
        "def preprocesar(texto):\n",
        "    texto = limpiar(texto)\n",
        "    texto = normalizar(texto)\n",
        "    texto = tokenizar(texto)\n",
        "\n",
        "    return texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXct2b4K5TSr"
      },
      "source": [
        "## Conjuntos de datos\n",
        "\n",
        "En la competencias normalmente encontramos dos archivos, el de entrenamiento y el de inferencia —que le suelen llamar de prueba y es el que tenemos que predecir para entregar—. Del que suelen llamar `train` también tenemos que obtener el de validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CIh7cXX5TSs"
      },
      "outputs": [],
      "source": [
        "infer_df = pd.read_csv('test.csv', sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esbaCji-5TSs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, valid_df = train_test_split(df, test_size=.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTojw4G15TSs"
      },
      "source": [
        "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3061129%2Fc9ef73f33bf2b505d48be10211ffa47c%2FScikit.jfif?generation=1580787055399439&alt=media)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwhvhBd5TSs"
      },
      "source": [
        "Ahora estamos en condiciones de pre-procesar todo lo que tenemos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc8GSI-e5TSs"
      },
      "outputs": [],
      "source": [
        "train_docs = [preprocesar(doc) for doc in train_df['Pregunta'].values]\n",
        "valid_docs = [preprocesar(doc) for doc in valid_df['Pregunta'].values]\n",
        "infer_docs = [preprocesar(doc) for doc in infer_df['Pregunta'].values]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fqRBjs05TSs"
      },
      "source": [
        "Hemos pasado de una Series de Pandas, array de NumPy o una **lista de strings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4gwg-mk5TSs",
        "outputId": "f7bd4a2a-512e-47a0-d2e2-6a54cbb16160"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['que se requiere para un préstamo personal?',\n",
              "       'me piden mi número de cuenta es mi cbu?',\n",
              "       'necesitar adherir aysa tarjeta',\n",
              "       'te financian igual un usado o un 0km?'], dtype=object)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['Pregunta'].values[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QoAA5CC5TSt"
      },
      "source": [
        "a una **lista de listas de strings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1My8dJ465TSt",
        "outputId": "04193ee4-72f6-45a0-c0c4-00f303741a5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['que', 'se', 'requiere', 'para', 'un', 'prestamo', 'personal'],\n",
              " ['me', 'piden', 'mi', 'numero', 'de', 'cuenta', 'es', 'mi', 'cbu'],\n",
              " ['necesitar', 'adherir', 'aysa', 'tarjeta'],\n",
              " ['te', 'financian', 'igual', 'un', 'usado', 'o', 'un', 'km']]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_docs[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqwc2jIk5TSt"
      },
      "source": [
        "Un poco de nomenclatura: estamos llamando corpus a la colección de textos. Nos referimos también a los textos como documentos. También estamos usando el término lote (*batch*) para referirnos a un (sub)conjunto de documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW0Dv99-5TSt"
      },
      "source": [
        "## Vocabulario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnM1otZ95TSt"
      },
      "source": [
        "Este paso es importante. Aquí definimos y limitamos la tókenes que vamos a utilizar. El lenguaje es infinito, para convertirlo en un problema tratable muchas veces los que hacemos es reducirlo. Clave para varias prácticas de reducción es contar las frecuencias de los tókenes, esto es, cuántas veces aparece cada tóken en todo el corpus. Como mencionamos las palabras más frecuentes no aportan mucha información y las más infrecuentes si bien son las que más información tienen no llegarán a ser representativas para nuestro modelo. Descartar palabras poco frecuentes también afecta a errores ortográficos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0HLi5Sk5TSt"
      },
      "source": [
        "Útil para este paso es la clase `Counter` de la librería estándar de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DCMqQGk5TSt",
        "outputId": "290364f6-7547-4f89-8b65-43cb15395cf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 3), ('b', 2)]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "c = Counter(['a','b','c','a','b','a'])\n",
        "\n",
        "# obtener los dos elementos más comunes y sus frecuencias\n",
        "c.most_common(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlG3b09Y5TSu"
      },
      "source": [
        "Una función de la librería estándar llamada `chain` nos dará una mano convirtiendo la lista de listas de tókenes en una lista de tókenes, similar a `numpy.flatten`, ya que `Counter` espera una lista con elementos a contar y nuestros tókenes están separados por documentos, hay que juntarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5pkr8iu5TSu",
        "outputId": "dae4eca2-136f-41b2-ca6e-1cc1bb73e62a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a', 'b', 'c', 'c', 'd']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from itertools import chain\n",
        "\n",
        "list(chain(['a','b','c'], ['c','d']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6I3K3J55TSu"
      },
      "source": [
        "`chain` encadena las listas que le pasamos como argumentos variables. Podemos usar el operador *splat* `*` para contentar a la función (convertir la lista principal en una serie de argumentos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYy--1lY5TSu",
        "outputId": "c949b183-0631-486a-c567-51a5f5158d82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a', 'b', 'c', 'c', 'd']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(chain( *[ ['a','b','c'], ['c','d'] ] ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22FrTItP5TSu"
      },
      "source": [
        "En vez de una lista podemos pedir un conjunto (`set`), en el que los elementos no se repiten. Este bien podría ser el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxlINvAB5TSu",
        "outputId": "5ff4d7a0-5155-4df3-ed60-fa42cf74a0d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a', 'b', 'c', 'd'}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set(chain( *[ ['a','b','c'], ['c','d'] ] ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBKOhwcT5TSu"
      },
      "source": [
        "En definitiva, es la lista oficial de tókenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UBAEboq5TSv"
      },
      "outputs": [],
      "source": [
        "# versión 1\n",
        "class Vocab():\n",
        "    def fit(self, lote):\n",
        "        self.vocabulario = set(chain(*lote))\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocabulario)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMxs91AG5TSv"
      },
      "source": [
        "Es importante generar el vocabulario con el dataset de entrenamiento, ya que como mencionamos se trata de la lista de palabras conocidas. Le agregamos un `__len__` porque también es útil conocer el tamaño del vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KbTXgsq5TSv",
        "outputId": "65f355b2-454a-44f7-89b5-055113e19fdd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_docs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-04b70b8e4c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_docs' is not defined"
          ]
        }
      ],
      "source": [
        "v = Vocab().fit(train_docs)\n",
        "len(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPtYyjm5TSv"
      },
      "source": [
        "¿Qué pasa con las palabras que no están en la lista? Se las conoce como tókenen **fuera del vocabulario** (*out-of-vocabulary*, abreviado OOV). Estas requieren acciones especiales, podríamos\n",
        "* ignorarlas\n",
        "* reemplazarlas por un tóken especial\n",
        "* inferirlas (ver más adelante, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufJSfiVA5TSv"
      },
      "outputs": [],
      "source": [
        "# versión 1.1\n",
        "class Vocab():\n",
        "    def __init__(self, tóken_desconocido='<unk>'):\n",
        "        self.tóken_desconocido = tóken_desconocido\n",
        "        \n",
        "    def fit(self, lote):\n",
        "        self.vocabulario = list(set(chain(*lote)))\n",
        "        \n",
        "        if self.tóken_desconocido:\n",
        "            self.vocabulario.append(self.tóken_desconocido)\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def transform(self, lote):\n",
        "        if self.tóken_desconocido: # reemplazar\n",
        "            return [[tóken if tóken in self.vocabulario else self.tóken_desconocido for tóken in doc] for doc in lote]\n",
        "        else: # ignorar\n",
        "            return [[tóken for tóken in doc if tóken in self.vocabulario] for doc in lote]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocabulario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMjrdNRB5TSv",
        "outputId": "90b8e4b3-af6e-4d95-f80d-d42734683daf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['poder', 'gestionar', 'clave', 'paso', 'pagina'],\n",
              " ['desde', 'cuando', '<unk>', 'con', 'el', 'programa', 'de', 'millas']]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Vocab().fit(train_docs).transform([\n",
        "    ['poder', 'gestionar', 'clave', 'paso', 'pagina'],\n",
        "    ['desde', 'cuando', 'arranco', 'con', 'el', 'programa', 'de', 'millas'],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOOaFGxV5TSw"
      },
      "source": [
        "## Numericalización\n",
        "\n",
        "También conocido como indexación. Así como a las unidades mínimas que consideramos las llamamos tókenes, a los números que los representan los llamamos **índices**. Ya que el vocabulario tiene la lista de tókenes, le vamos a pedir una responsabilidad adicional: que mantenga una asignación entre tókenes y números enteros. Posiblemente ya te ha sucedido pasarle valores no númericos a un estimador y ver cómo falla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rMwq-au5TSw",
        "outputId": "ce8a9715-89af-4261-dba2-ace6ead5fd85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a': 0, 'b': 1, 'c': 2, 'd': 3, '<unk>': 4}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulario = ['a','b','c','d','<unk>']\n",
        "\n",
        "{tóken: índice for índice, tóken in enumerate(vocabulario)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJAZ24Ss5TSw"
      },
      "source": [
        "que es lo mismo que"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7YJ52jr5TSw",
        "outputId": "991f5bc9-8437-4a7d-dc95-dcd2b95e49b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'a': 0, 'b': 1, 'c': 2, 'd': 3, '<unk>': 4}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mapeo = {}\n",
        "\n",
        "for índice, tóken in enumerate(vocabulario):\n",
        "    mapeo[tóken] = índice\n",
        "\n",
        "mapeo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt_2z7qu5TSw"
      },
      "source": [
        "¿Qué es lo que hace `enumerate`? Como su nombre lo indica, enumera los elementos de una colección."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Ht2jz25TSw",
        "outputId": "a73c1b87-af08-4ff7-c063-7a1d40792025"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, '<unk>')]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(enumerate(vocabulario))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnyNsMEO5TSw"
      },
      "outputs": [],
      "source": [
        "# versión 2\n",
        "class Vocab():\n",
        "    def __init__(self, tóken_desconocido='<unk>'):\n",
        "        self.tóken_desconocido = tóken_desconocido\n",
        "        \n",
        "    def fit(self, lote):\n",
        "        vocabulario = list(set(chain(*lote)))\n",
        "        \n",
        "        if self.tóken_desconocido:\n",
        "            vocabulario.append(self.tóken_desconocido)\n",
        "        \n",
        "        self.mapeo = {tóken: índice for índice, tóken in enumerate(vocabulario)}\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def transform(self, lote):\n",
        "        if self.tóken_desconocido: # reemplazar\n",
        "            return [[tóken if tóken in self.mapeo else self.tóken_desconocido for tóken in doc] for doc in lote]\n",
        "        else: # ignorar\n",
        "            return [[tóken for tóken in doc if tóken in self.mapeo] for doc in lote]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.mapeo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diQV1z6a5TSx"
      },
      "source": [
        "Comprobemos que la nueva versión de `Vocab` funciona como la anterior. Además veamos qué sucedo cuando no queremos el tóken para palabras fuera de vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYN8_gMP5TSx",
        "outputId": "19dd06d9-e7e4-40e6-8b7b-375854fd7a4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['poder', 'gestionar', 'clave', 'paso', 'pagina'],\n",
              " ['desde', 'cuando', 'con', 'el', 'programa', 'de', 'millas']]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Vocab(tóken_desconocido=None).fit(train_docs).transform([\n",
        "    ['poder', 'gestionar', 'clave', 'paso', 'pagina'],\n",
        "    ['desde', 'cuando', 'arranco', 'con', 'el', 'programa', 'de', 'millas'],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExH8P6sF5TSx"
      },
      "source": [
        "Ahora vamos a agregar métodos para convertir tókenes a índices y viceversa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1CGBYeN5TSx"
      },
      "outputs": [],
      "source": [
        "# versión 2.1\n",
        "class Vocab():\n",
        "    def __init__(self, tóken_desconocido='<unk>'):\n",
        "        self.tóken_desconocido = tóken_desconocido\n",
        "        \n",
        "    def fit(self, lote):\n",
        "        # agregamos `sorted` porque el orden al aplicar `set` no está asegurado\n",
        "        vocabulario = list(sorted(set(chain(*lote))))\n",
        "        \n",
        "        if self.tóken_desconocido:\n",
        "            vocabulario.append(self.tóken_desconocido)\n",
        "        \n",
        "        self.mapeo = {tóken: índice for índice, tóken in enumerate(vocabulario)}\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def transform(self, lote):\n",
        "        if self.tóken_desconocido: # reemplazar\n",
        "            return [[tóken if tóken in self.mapeo else self.tóken_desconocido for tóken in doc] for doc in lote]\n",
        "        else: # ignorar\n",
        "            return [[tóken for tóken in doc if tóken in self.mapeo] for doc in lote]\n",
        "    \n",
        "    def tókenes_a_índices(self, lote):\n",
        "        lote = self.transform(lote)\n",
        "        \n",
        "        return [[self.mapeo[tóken] for tóken in doc] for doc in lote]\n",
        "    \n",
        "    def índices_a_tókenes(self, lote):\n",
        "        mapeo_inverso = list(self.mapeo.keys())\n",
        "        \n",
        "        return [[mapeo_inverso[índice] for índice in doc] for doc in lote]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.mapeo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-5pIzju5TSy",
        "outputId": "3b6ecd7e-1444-449f-a3bd-1e8fcf499d71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[4160, 4683, 4484, 3703, 5294, 4011, 3825],\n",
              " [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950]]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v = Vocab(tóken_desconocido=None).fit(train_docs)\n",
        "\n",
        "v.tókenes_a_índices([\n",
        "    ['que', 'se', 'requiere', 'para', 'un', 'prestamo', 'personal'],\n",
        "    ['me', 'piden', 'mi', 'numero', 'de', 'cuenta', 'es', 'mi', 'cbu'],\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtRcjZ355TSy",
        "outputId": "d117e5d1-cd51-4b33-d199-e2361f4bf83f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['que', 'se', 'requiere', 'para', 'un', 'prestamo', 'personal'],\n",
              " ['me', 'piden', 'mi', 'numero', 'de', 'cuenta', 'es', 'mi', 'cbu']]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v.índices_a_tókenes([\n",
        "    [4160, 4683, 4484, 3703, 5294, 4011, 3825],\n",
        "    [3275, 3854, 3319, 3554, 1532, 1462, 2151, 3319, 950],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Ot7PmL5TSy"
      },
      "source": [
        "## Casos especiales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_91SjD-5TSy"
      },
      "source": [
        "¿Qué sucede con los documentos que al ser tokenizados regresan vacíos? ¿O con documentos compuestos enteramente por palabras fuera del vocabulario?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fG7a-bR5TSy",
        "outputId": "05ba2ab8-b2df-4eb1-d782-a4a58b0b7cb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[], ['banks', 'charge', 'high', 'fees', 'for', 'foreign', 'atm']]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documentos_problemáticos = [\n",
        "    '??? ???',\n",
        "    'Banks charge high fees for foreign ATM'\n",
        "]\n",
        "\n",
        "[preprocesar(doc) for doc in documentos_problemáticos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I4xaPjm5TSy",
        "outputId": "19ce273d-05b5-4794-b6d1-3373e34d5965"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[], ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v = Vocab().fit(train_docs)\n",
        "\n",
        "v.transform([[], ['banks', 'charge', 'high', 'fees', 'for', 'foreign']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BokVowhS5TSy",
        "outputId": "24615fe6-68b7-45a4-f85f-8b39fe728d99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[], [5583, 5583, 5583, 5583, 5583, 5583]]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v.tókenes_a_índices([[], ['banks', 'charge', 'high', 'fees', 'for', 'foreign']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt2sszvN5TSz",
        "outputId": "447fdd59-7227-466d-a931-064e39b52adb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[], ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v.índices_a_tókenes([[], [5583, 5583, 5583, 5583, 5583, 5583]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP0bnDb-5TSz"
      },
      "source": [
        "La conclusión es que [no pasa nada](https://www.youtube.com/watch?v=UiZWxx1i6iA) (al menos por ahora)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBuRhC__5TSz"
      },
      "source": [
        "## Bonus: reducción del vocabulario\n",
        "\n",
        "La idea es limitar los tókenes que vamos a utilizar. En cierta forma cada tóken es un atributo (*feature*) y quisiéramos proveer atributos que sean de utilidad para el estimador.\n",
        "\n",
        "El lenguaje es infinito, para convertirlo en un problema tratable muchas veces los que hacemos es reducirlo. Clave para varias prácticas de reducción es contar las frecuencias de los tókenes, esto es, cuántas veces aparece cada tóken en todo el corpus. Como mencionamos las palabras más frecuentes no aportan mucha información y las más infrecuentes si bien son las que más información tienen no llegarán a ser representativas para nuestro modelo. Descartar palabras poco frecuentes también afecta a errores ortográficos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13iF9AtD5TSz"
      },
      "source": [
        "Útil para este paso es la clase `Counter` de la librería estándar de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icrFvv2S5TSz",
        "outputId": "ed2fe207-1716-4b53-9140-a7ec6a23a9fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 3), ('b', 2), ('c', 1)]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "c = Counter(['a','b','c','a','b','a'])\n",
        "\n",
        "# obtener los elementos ordenados de más comunes a menos\n",
        "c.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWKPuP85TSz"
      },
      "source": [
        "Acerca de contar palabras, no te pierdas la [ley de Zipf](https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb#(2)-Models:-Bag-of-Words)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKEPH8o15TSz"
      },
      "source": [
        "### Más comunes\n",
        "\n",
        "Una estrategia simple es ordenar a los tókenes según frecuencia y poner un límite duro al vocabulario, de modo de quedarnos con los `límite` más comunes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c79j8npV5TSz",
        "outputId": "b843f331-1d85-4957-9c99-c71de56b6c84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a', 'b']"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "límite = 2\n",
        "\n",
        "vocabulario = list(c)[:límite]\n",
        "vocabulario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtLhpQkC5TS0"
      },
      "source": [
        "### Por frecuencia de tóken\n",
        "\n",
        "Podríamos descartar los que aparecen\n",
        "* más de `máximo` veces,\n",
        "* menos de `mínimo` veces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHxcylOd5TS0",
        "outputId": "d3b219a2-1796-43ba-ae9c-c6a5517917c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a', 'b']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "máximo = 3\n",
        "mínimo = 2\n",
        "\n",
        "vocabulario = [tóken for tóken, frecuencia in c.most_common() if máximo >= frecuencia >= mínimo]\n",
        "vocabulario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kto08m85TS0"
      },
      "source": [
        "### Por frecuencia de documento\n",
        "\n",
        "O bien, en vez de contar las apariciones absolutas, contar en cuántos documentos aparece cada tóken. Un tóken que aparezca en todos los documentos no colaboraría en una tarea de clasificación, a distinguir documentos pero uno que aparezca en la mitad de los documentos podría ser útil para separarlos en dos grupos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMYOC7nu5TS0",
        "outputId": "85369b23-dd49-4f13-b851-ad8cb58bd6df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('hola', 2), ('buen', 1), ('día', 1), ('tardes', 1), ('buenas', 1)]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c = Counter()\n",
        "\n",
        "lote = [\n",
        "    ['hola', 'buen', 'día'],\n",
        "    ['hola', 'buenas', 'tardes'],\n",
        "]\n",
        "\n",
        "for doc in lote:\n",
        "    c.update(set(doc))\n",
        "\n",
        "c.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9stXQbPH5TS0"
      },
      "source": [
        "Vamos a normalizar la frecuencias por la cantidad total de documentos ($D$) y de manera similar al punto anterior podríamos descartar los elementos que aparecen en:\n",
        "* más del `máximo` proporción de los documentos.\n",
        "* menos del `mínimo` proporción de los documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGwJoC7f5TS0",
        "outputId": "c657dd05-cf64-4d49-ecef-7fa071fe15a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['buen', 'día', 'tardes', 'buenas']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "D = len(lote)\n",
        "\n",
        "máximo = .9\n",
        "mínimo = .1\n",
        "\n",
        "vocabulario = [tóken for tóken, frecuencia in c.most_common() if máximo >= frecuencia/D >= mínimo]\n",
        "vocabulario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bn-1Fuu5TS1"
      },
      "source": [
        "### *Stop words*\n",
        "\n",
        "Hay listas armadas de palabras muy comunes (*stop words*). Podemos elaborarla de alguna manera o usar alguna existente.\n",
        "\n",
        "    pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7TkUzpw5TS1",
        "outputId": "6bfc9c68-1b6e-4538-9471-877d0737b393"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/matias/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "    \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords.words('spanish')[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJVoIDuL5TS1"
      },
      "source": [
        "Un detalle a cuidar es que la tokenización usada para la lista de *stop words* tiene que haber sido la misma o similar que la usada para los documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0PuDtWk5TS1",
        "outputId": "c6368e4f-01ec-4986-cccb-e1a740d1fd1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['requiere', 'prestamo', 'personal'], ['piden', 'numero', 'cuenta', 'cbu']]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def filtrar_stop_words(lote):\n",
        "    return [[tóken for tóken in doc if tóken not in stopwords.words('spanish')] for doc in lote]\n",
        "\n",
        "filtrar_stop_words([\n",
        "    ['que', 'se', 'requiere', 'para', 'un', 'prestamo', 'personal'],\n",
        "    ['me', 'piden', 'mi', 'numero', 'de', 'cuenta', 'es', 'mi', 'cbu'],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6U4RNLO5TS1"
      },
      "source": [
        "### Por longitud\n",
        "\n",
        "Esta técnica no requiere contar la frecuencia de los tókenes, simplemente filtramos tókenes muy cortos o muy largos ya que en general son ruidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZEUG0-b5TS1",
        "outputId": "bea151c2-17c5-407f-b410-2b73dbeeec1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['que', 'requiere', 'para', 'prestamo', 'personal'],\n",
              " ['piden', 'numero', 'cuenta', 'cbu']]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def filtrar_por_longitud(lote, máxima, mínima):\n",
        "    return [[tóken for tóken in doc if máxima >= len(tóken) >= mínima] for doc in lote]\n",
        "\n",
        "filtrar_por_longitud([\n",
        "    ['que', 'se', 'requiere', 'para', 'un', 'prestamo', 'personal'],\n",
        "    ['me', 'piden', 'mi', 'numero', 'de', 'cuenta', 'es', 'mi', 'cbu'],\n",
        "], máxima=9, mínima=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9huyb6Q5TS1"
      },
      "source": [
        "### Implementación\n",
        "\n",
        "Veamos cómo acomodamos lo que hemos visto ahora en la clase `Vocab`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpnZyyWq5TS1"
      },
      "outputs": [],
      "source": [
        "# versión 3\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self, tóken_desconocido='<unk>', frecuencia_mínima=0.0, frecuencia_máxima=1.0,\n",
        "                 longitud_mínima=1, longitud_máxima=np.inf, stop_words=[], límite_vocabulario=None):\n",
        "        \n",
        "        self.tóken_desconocido = tóken_desconocido\n",
        "        self.frecuencia_mínima = frecuencia_mínima\n",
        "        self.frecuencia_máxima = frecuencia_máxima\n",
        "        self.longitud_mínima = longitud_mínima\n",
        "        self.longitud_máxima = longitud_máxima\n",
        "        self.stop_words = stop_words\n",
        "        self.límite_vocabulario = límite_vocabulario\n",
        "    \n",
        "    def reducir_vocabulario(self, lote):\n",
        "        contador_absoluto = Counter(chain(*lote))\n",
        "        \n",
        "        contador_documentos = Counter()\n",
        "        \n",
        "        for doc in lote:\n",
        "            contador_documentos.update(set(doc))\n",
        "        \n",
        "        # frecuencia mínima\n",
        "        if isinstance(self.frecuencia_mínima, int): # frecuencia de tóken\n",
        "            vocabulario_mín = [tóken for tóken, frecuencia in contador_absoluto.most_common() if frecuencia >= self.frecuencia_mínima]\n",
        "        else: # frecuencia de documento\n",
        "            vocabulario_mín = [tóken for tóken, frecuencia in contador_documentos.most_common() if frecuencia/len(lote) >= self.frecuencia_mínima]\n",
        "        \n",
        "        # frecuencia máxima\n",
        "        if isinstance(self.frecuencia_máxima, int): # frecuencia de tóken\n",
        "            vocabulario_máx = [tóken for tóken, frecuencia in contador_absoluto.most_common() if self.frecuencia_máxima >= frecuencia]\n",
        "        else: # frecuencia de documento\n",
        "            vocabulario_máx = [tóken for tóken, frecuencia in contador_documentos.most_common() if self.frecuencia_máxima >= frecuencia/len(lote)]\n",
        "\n",
        "        # intersección de vocabulario_mín y vocabulario_máx preservando el órden\n",
        "        vocabulario = [tóken for tóken in vocabulario_mín if tóken in vocabulario_máx]\n",
        "\n",
        "        # longitud\n",
        "        vocabulario = [tóken for tóken in vocabulario if self.longitud_máxima >= len(tóken) >= self.longitud_mínima]\n",
        "        \n",
        "        # stop words\n",
        "        vocabulario = [tóken for tóken in vocabulario if tóken not in self.stop_words]\n",
        "        \n",
        "        # límite\n",
        "        vocabulario = vocabulario[:self.límite_vocabulario]\n",
        "        \n",
        "        return vocabulario\n",
        "        \n",
        "    def fit(self, lote):\n",
        "        vocabulario = self.reducir_vocabulario(lote)\n",
        "        \n",
        "        if self.tóken_desconocido:\n",
        "            vocabulario.append(self.tóken_desconocido)\n",
        "        \n",
        "        self.mapeo = {tóken: índice for índice, tóken in enumerate(vocabulario)}\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def transform(self, lote):\n",
        "        if self.tóken_desconocido: # reemplazar\n",
        "            return [[tóken if tóken in self.mapeo else self.tóken_desconocido for tóken in doc] for doc in lote]\n",
        "        else: # ignorar\n",
        "            return [[tóken for tóken in doc if tóken in self.mapeo] for doc in lote]\n",
        "    \n",
        "    def tókenes_a_índices(self, lote):\n",
        "        lote = self.transform(lote)\n",
        "        \n",
        "        return [[self.mapeo[tóken] for tóken in doc] for doc in lote]\n",
        "    \n",
        "    def índices_a_tókenes(self, lote):\n",
        "        mapeo_inverso = list(self.mapeo.keys())\n",
        "        \n",
        "        return [[mapeo_inverso[índice] for índice in doc] for doc in lote]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mapeo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y84X-4Vv5TS2",
        "outputId": "4943d1a0-1139-46b6-f8dc-ad8b2bee0873"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['poder', 'gestionar', 'clave', 'paso', 'pagina'],\n",
              " ['desde', 'cuando', '<unk>', 'con', '<unk>', 'programa', '<unk>', 'millas']]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Vocab(longitud_mínima=3).fit(train_docs).transform([\n",
        "    ['poder', 'gestionar', 'clave', 'paso', 'pagina'],\n",
        "    ['desde', 'cuando', 'arranco', 'con', 'el', 'programa', 'de', 'millas'],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaH3jrO35TS2"
      },
      "source": [
        "## El pre-procesamiento hasta ahora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmSqBKdi5TS2"
      },
      "outputs": [],
      "source": [
        "v = Vocab().fit(train_docs)\n",
        "\n",
        "train_índices = v.tókenes_a_índices(train_docs)\n",
        "valid_índices = v.tókenes_a_índices(valid_docs)\n",
        "infer_índices = v.tókenes_a_índices(infer_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLfbs4m5TS2"
      },
      "source": [
        "Con esto concluye la primera parte. Hay varias librerías que tienen clases que se encargan de efectuar los pasos que hemos visto. Tienen un comportamiento por defecto, que es configurable (los parámetros que hemos visto) y a su vez, personalizable, para reemplazar algunos o todos los pasos por código propio. En general son librerías desarrolladas por angloparlantes, funcionan *out-of-the-box* bien para el inglés; cuando queremos procesar texto en español vale la pena tener más control sobre estos procesos.\n",
        "\n",
        "* [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de scikit-learn.\n",
        "* [TextDataBunch](https://docs.fast.ai/text.data.html#TextDataBunch.from_df) de fast.ai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rf94M9i5TS2"
      },
      "source": [
        "## Pre-procesando las etiquetas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihZc6Mbe5TS2"
      },
      "source": [
        "Las etiquetas del dataset también necesitan ser convertidas a números enteros consecutivos. No lo pensamos para este fin pero `Vocab` sería útil en este aspecto. El único tema es que `Vocab.fit` y demás métodos esperan listas de listas de tókenes y a las etiquetas las encontramos en forma de listas de tókenes simplemente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSJkWT5J5TS2",
        "outputId": "0e755886-9283-487e-bbe8-fb67252aa29f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Cat_248', 'Cat_42', 'Cat_132', ..., 'Cat_293', 'Cat_138',\n",
              "       'Cat_219'], dtype=object)"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df['Intencion'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edWw7hm85TS3"
      },
      "source": [
        "Podemos llevar la columna de las etiquetas a una lista de listas con `train_df['Intencion'].values.reshape(-1,1)`, de manera de poder interfacearlo con `Vocab`. Algo como `train_df[['Intencion']].values` para que Pandas devuelva un `DataFrame` en vez de una `Series` también funcionaría."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx2FPB6T5TS3",
        "outputId": "43d251b4-286b-42d7-99b3-66a7f7f8651c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['Cat_248'],\n",
              "       ['Cat_42'],\n",
              "       ['Cat_132'],\n",
              "       ...,\n",
              "       ['Cat_293'],\n",
              "       ['Cat_138'],\n",
              "       ['Cat_219']], dtype=object)"
            ]
          },
          "execution_count": 310,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_etiquetas = train_df[['Intencion']].values\n",
        "valid_etiquetas = valid_df[['Intencion']].values\n",
        "train_etiquetas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir94eLey5TS3"
      },
      "source": [
        "Todo lo que tenga que ver con limitación del vocabulario o agregado de tókenes especiales no nos interesa para este caso de uso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-F6b9d95TS3",
        "outputId": "422490e0-5675-4987-8c1b-e8b23aefe77f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[6], [128], [0], [104], [6], [17], [8], [202], [306], [166]]"
            ]
          },
          "execution_count": 314,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulario_etiquetas = Vocab(tóken_desconocido=None).fit(train_etiquetas)\n",
        "\n",
        "train_etiquetas = vocabulario_etiquetas.tókenes_a_índices(train_etiquetas)\n",
        "valid_etiquetas = vocabulario_etiquetas.tókenes_a_índices(valid_etiquetas)\n",
        "\n",
        "train_etiquetas[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBMGgESH5TS3"
      },
      "source": [
        "Ya casi estamos. Solo debemos reconvertir a las etiquetas en una lista de índices (su dimensión original) con un recurso que ya conocemos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf6hIydG5TS3",
        "outputId": "3eb8d40e-2026-4860-cc08-aa6a9fadab9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[6, 128, 0, 104, 6, 17, 8, 202, 306, 166]"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_etiquetas = list(chain(*train_etiquetas))\n",
        "valid_etiquetas = list(chain(*valid_etiquetas))\n",
        "\n",
        "train_etiquetas[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAdqbXtt5TS3"
      },
      "source": [
        "Ahora estás en condiciones de seguir con la [segunda parte](Preprocesamiento-de-texto-para-NLP-parte-2.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Uqgp1L5TS3"
      },
      "source": [
        "## Fuentes consultadas\n",
        "\n",
        "* http://anie.me/On-Torchtext/\n",
        "* https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "name": "pytorch-gpu.1-4.m50",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNskWEKwj7xfOUBaUfA/Ozd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/txusser/Master_IA_Sanidad/blob/main/Modulo_3/NLP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalamos algunas dependencias."
      ],
      "metadata": {
        "id": "VVHdPxQ2mzJc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er3LAaC_de9Z",
        "outputId": "1504d484-0a9b-4a42-a47d-ec586e5731c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘tutorial’: File exists\n",
            "fatal: destination path 'scikit-learn' already exists and is not an empty directory.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n"
          ]
        }
      ],
      "source": [
        "!mkdir tutorial\n",
        "!git clone https://github.com/scikit-learn/scikit-learn.git\n",
        "!pip install scikit-learn\n",
        "!cp -r scikit-learn/doc/tutorial/text_analytics/ tutorial\n",
        "!python tutorial/text_analytics/data/twenty_newsgroups/fetch_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos descargamos el dataset que vamos a utilizar en este ejemplo. \n",
        "Es un dataset de e-mails (aproximadamente 12.000 documentos en total), divididos (casi) uniformemente en 20 grupos de dependiendo de los temas que se tratan. Fue recopilado originalmente por Ken Lang, probablemente para su artículo \"Newsweeder: Learning to filter netnews\"."
      ],
      "metadata": {
        "id": "UxdxM3tam2d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train')\n",
        "twenty_train.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaOOU0yThwVK",
        "outputId": "fe458174-66ac-4df8-bdeb-0a28c9248cde"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(twenty_train.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Ibf9SEkl3N",
        "outputId": "b3e3508c-5a82-4e9b-c06a-2f48a784e8ee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in twenty_train.target[:10]:\n",
        "  print(twenty_train.target_names[t])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YiX4hTPkoYS",
        "outputId": "1dcddcfe-7700-4408-98dc-7d09706fbe7e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rec.autos\n",
            "comp.sys.mac.hardware\n",
            "comp.sys.mac.hardware\n",
            "comp.graphics\n",
            "sci.space\n",
            "talk.politics.guns\n",
            "sci.med\n",
            "comp.sys.ibm.pc.hardware\n",
            "comp.os.ms-windows.misc\n",
            "comp.sys.mac.hardware\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(twenty_train.data[4].split(\"\\n\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPLB6Pr4k53J",
        "outputId": "056a9597-5f8d-4d19-899b-94426f5b16e4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\n",
            "Subject: Re: Shuttle Launch Question\n",
            "Organization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\n",
            "Distribution: sci\n",
            "Lines: 23\n",
            "\n",
            "From article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\n",
            ">>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\n",
            ">>>\"Clear caution & warning memory.  Verify no unexpected\n",
            ">>>errors. ...\".  I am wondering what an \"expected error\" might\n",
            ">>>be.  Sorry if this is a really dumb question, but\n",
            "> \n",
            "> Parity errors in memory or previously known conditions that were waivered.\n",
            ">    \"Yes that is an error, but we already knew about it\"\n",
            "> I'd be curious as to what the real meaning of the quote is.\n",
            "> \n",
            "> tom\n",
            "\n",
            "\n",
            "My understanding is that the 'expected errors' are basically\n",
            "known bugs in the warning system software - things are checked\n",
            "that don't have the right values in yet because they aren't\n",
            "set till after launch, and suchlike. Rather than fix the code\n",
            "and possibly introduce new bugs, they just tell the crew\n",
            "'ok, if you see a warning no. 213 before liftoff, ignore it'.\n",
            "\n",
            " - Jonathan\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‎El preprocesamiento de texto, la tokenización y el filtrado de palabras clave se incluyen en ‎‎CountVectorizer‎‎, que crea un diccionario de características y transforma los emails en vectores de características:.\n",
        "\n"
      ],
      "metadata": {
        "id": "BUY4B2CZmHSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "X_train_counts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1IcAbgEmGBN",
        "outputId": "f6898fd8-27c9-4fb8-ade7-f343def06450"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 130107)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el tamaño de nuestro vocabulario"
      ],
      "metadata": {
        "id": "eBC7fvLYnmhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect.vocabulary_.get(u'algorithm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSgE1nRKmcoA",
        "outputId": "dc161cca-4c27-4771-a4ef-a77bd2e33d4c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27366"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‎El recuento de ocurrencias es un buen comienzo, pero hay un problema: los documentos más largos tendrán valores de recuento promedio más altos que los documentos más cortos, aunque puedan hablar sobre los mismos temas.‎\n",
        "\n",
        "‎Para evitar estas posibles discrepancias basta con dividir el número de apariciones de cada palabra en un documento por el número total de palabras en el documento: estas nuevas características se denominan Frecuencias de Término (tf).\n",
        "\n",
        "‎Otro refinamiento además de transformar los valores en tf es reducir los pesos de las palabras que aparecen en muchos documentos del corpus y, por lo tanto, son menos informativas que las que ocurren solo en una porción más pequeña del corpus.‎\n",
        "\n",
        "‎Esta reducción de escala se denomina ‎‎tf-idf‎‎ para \"Term Frequency times Inverse Document Frequency\".‎\n",
        "\n",
        "‎Tanto ‎‎tf‎‎ como ‎‎tf-idf‎‎ se pueden calcular de la siguiente manera utilizando ‎‎TfidfTransformer‎‎:‎"
      ],
      "metadata": {
        "id": "ckXmYRmFmwmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
        "X_train_tf = tf_transformer.transform(X_train_counts)"
      ],
      "metadata": {
        "id": "nFMQVy0Wmv3C"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‎En el código de ejemplo anterior, en primer lugar usamos el método \"*fit*\" para ajustar nuestro estimador a los datos y, en segundo lugar, el método \"*transform*\" para transformar nuestra matriz de conteo en una representación tf-idf. Estos dos pasos se pueden combinar para lograr el mismo resultado final al omitir el procesamiento redundante. Esto se hace mediante el uso del método \"*fit_transform*\". Ejercicio: Reemplazar por el método \"*fit_transform*\""
      ],
      "metadata": {
        "id": "e6WF2nGXpeUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgYViQgKragf",
        "outputId": "dbca75d7-1ae3-44ce-d95a-5d676ce08436"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 130107)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‎Ahora que tenemos nuestras features, podemos entrenar un clasificador para que intente predecir la categoría de un post. Comencemos con un clasificador ‎‎naive Bayes, que proporciona una buena línea de base para esta tarea. Existen varias variantes de este clasificador; la más adecuada para el recuento de palabras es la variante multinomial"
      ],
      "metadata": {
        "id": "Tuoio1urp69F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
      ],
      "metadata": {
        "id": "LdbUVaE6psDm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para tratar de predecir el resultado en un nuevo documento, necesitamos extraer las características utilizando casi la misma cadena de extracción de características que antes. La diferencia es que llamamos a \"*transform*\" en lugar de a \"*fit_transform*\", ya que ya se han ajustado al conjunto de entrenamiento"
      ],
      "metadata": {
        "id": "9AzXLVMWqhec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_new = ['Jesus Loves You. Follow his guidance and you will be able to reach heaven', 'Earth is 7 light years away from the sun','You need to ensure that the engine temperature does not reach above 90 degrees']\n",
        "X_new_counts = count_vect.transform(docs_new)\n",
        "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
        "\n",
        "predicted = clf.predict(X_new_tfidf)\n",
        "\n",
        "for doc, category in zip(docs_new, predicted):\n",
        "  print('%r => %s' % (doc, twenty_train.target_names[category]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxTyIfRWqzDe",
        "outputId": "364800e8-d693-47ee-859b-55bc54e3f0f8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Jesus Loves You. Follow his guidance and you will be able to reach heaven' => soc.religion.christian\n",
            "'Earth is 7 light years away from the sun' => sci.space\n",
            "'You need to ensure that the engine temperature does not reach above 90 degrees' => rec.autos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XEr0slHIr6hw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}